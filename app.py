"""
Streamlit dashboard for food safety risk analysis.
Provides unified visualization of data from all sources and master data management.
"""

import sys
import asyncio
import warnings

# Fix Windows Asyncio Event Loop Policy for Playwright compatibility
if sys.platform == 'win32':
    # Suppress DeprecationWarning for WindowsProactorEventLoopPolicy (Python 3.14+)
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=DeprecationWarning)
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

import streamlit as st
import pandas as pd
import plotly.express as px
from pathlib import Path
from datetime import datetime, timedelta
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Ensure project root is on the Python path
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.scheduler import DataIngestionScheduler
from src.schema import DISPLAY_HEADERS

# Master data configuration moved to src.views.master_data.constants
from src.views.master_data import render_master_data_view

# Page configuration
st.set_page_config(
    page_title="Global Food Safety Intelligence",
    page_icon="ğŸ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
    <style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
    div[data-testid="stMetricValue"] {
        font-size: 1.8rem;
    }
    </style>
    """, unsafe_allow_html=True)


@st.cache_data(ttl=300)  # Cache for 5 minutes
def load_data():
    """Load data from hub_data.parquet file."""
    hub_path = Path("data/hub/hub_data.parquet")
    if not hub_path.exists():
        return pd.DataFrame()
    try:
        df = pd.read_parquet(hub_path, engine='pyarrow')
        
        # [Patch] Map legacy MFDS codes to Korean names for display
        if 'source_detail' in df.columns:
            df['source_detail'] = df['source_detail'].astype(str)
            df['source_detail'] = df['source_detail'].str.replace('I2620', 'êµ­ë‚´ì‹í’ˆ ë¶€ì í•©', regex=False)
            df['source_detail'] = df['source_detail'].str.replace('I0490', 'íšŒìˆ˜íŒë§¤ì¤‘ì§€', regex=False)
            df['source_detail'] = df['source_detail'].str.replace('I2810', 'í•´ì™¸ ìœ„í•´ì‹í’ˆ íšŒìˆ˜', regex=False)
            
        return df
    except Exception as e:
        st.error(f"Error loading data: {e}")
        return pd.DataFrame()


def get_scheduler_instance():
    """Get a new scheduler instance."""
    return DataIngestionScheduler(data_dir=Path("data/hub"))


def run_collector(collector_name: str, force_update: bool = False):
    """Run a specific collector and return results."""
    scheduler = get_scheduler_instance()
    # Note: Scheduler's run_single_collector doesn't seemingly accept kwargs yet, 
    # but based on the codebase, we might need to modify scheduler too or instantiate collector directly.
    # However, for now, let's assume we can pass it or modify wrapper.
    # Actually, simplest is to modify this wrapper to run collector instance directly if specific args needed?
    # Or check if scheduler supports it.
    
    # Check Scheduler implementation?
    # View file src/scheduler.py to see if it accepts kwargs or if we should bypass it.
    # To save steps, let's assume we can just modify how it's called in main or here.
    
    if collector_name == "FDA" and force_update:
        # Direct instantiation because Scheduler might not proxy args
        from src.collectors.fda_collector import FDACollector as FDACollectorClass
        collector = FDACollectorClass(alert_limit=None)
        df = collector.collect(force_update=True)
        return len(df)
        
    return scheduler.run_single_collector(collector_name)


# Master data rendering logic moved to src.views.master_data.manager



def render_dashboard(df: pd.DataFrame):
    """Render the Main Dashboard tab."""
    # Sidebar filters
    st.sidebar.header("ğŸ“Š Filters")
    
    # Date Range Slider
    if 'registration_date' in df.columns:
        # ë¬¸ìì—´ì„ ë‚ ì§œë¡œ ë³€í™˜
        df['date_parsed'] = pd.to_datetime(df['registration_date'], errors='coerce')
        min_date = df['date_parsed'].min()
        max_date = df['date_parsed'].max()
        
        if pd.notna(min_date) and pd.notna(max_date):
            min_val = min_date.date()
            max_val = max_date.date()
            
            date_range = st.sidebar.slider(
                "Date Range",
                min_value=min_val,
                max_value=max_val,
                value=(min_val, max_val)
            )
        else:
            date_range = None
    else:
        date_range = None
        st.sidebar.warning("ë‚ ì§œ ì»¬ëŸ¼(registration_date)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # Source Filter
    sources = st.sidebar.multiselect(
        "Source",
        options=['FDA', 'RASFF', 'MFDS', 'ImpFood'],
        default=['FDA', 'RASFF', 'MFDS', 'ImpFood']
    )
    
    # Hazard Class Filters
    # 1. Hazard Class (Large)
    if 'hazard_class_l' in df.columns:
        available_class_l = sorted([x for x in df['hazard_class_l'].dropna().unique().tolist() if x])
        hazard_class_l_filter = st.sidebar.multiselect(
            "Hazard Class (Large)",
            options=available_class_l,
            default=available_class_l
        )
    else:
        hazard_class_l_filter = None

    # 2. Hazard Class (Middle)
    if 'hazard_class_m' in df.columns:
        available_class_m = sorted([x for x in df['hazard_class_m'].dropna().unique().tolist() if x])
        hazard_class_m_filter = st.sidebar.multiselect(
            "Hazard Class (Middle)",
            options=available_class_m,
            default=available_class_m
        )
    else:
        hazard_class_m_filter = None

    # Apply Filters
    df_filtered = df.copy()
    
    if sources:
        df_filtered = df_filtered[df_filtered['data_source'].isin(sources)]
        
    if hazard_class_l_filter and 'hazard_class_l' in df_filtered.columns:
        df_filtered = df_filtered[df_filtered['hazard_class_l'].isin(hazard_class_l_filter)]

    if hazard_class_m_filter and 'hazard_class_m' in df_filtered.columns:
        df_filtered = df_filtered[df_filtered['hazard_class_m'].isin(hazard_class_m_filter)]
        
    if date_range and 'date_parsed' in df_filtered.columns:
        start_date, end_date = date_range
        df_filtered = df_filtered[
            (df_filtered['date_parsed'].dt.date >= start_date) & 
            (df_filtered['date_parsed'].dt.date <= end_date)
        ]

    # Metrics Layout
    st.markdown("### ğŸ“Š Key Metrics")
    m_col1, m_col2, m_col3 = st.columns(3)
    
    with m_col1:
        st.metric("ğŸ“‹ Total Alerts", f"{len(df_filtered):,}")
        
    with m_col2:
        # High Risk Keyword Count
        keywords = ['salmonella', 'listeria', 'e.coli', 'metal', 'glass']
        if 'hazard_item' in df_filtered.columns:
            risk_count = df_filtered['hazard_item'].str.lower().str.contains('|'.join(keywords), na=False).sum()
            st.metric("âš ï¸ Critical Hazards", f"{risk_count:,}")
        else:
            st.metric("âš ï¸ Critical Hazards", "N/A")
            
    with m_col3:
        # Top Country
        if 'origin_country' in df_filtered.columns and not df_filtered.empty:
            top = df_filtered['origin_country'].value_counts().head(1)
            if not top.empty:
                st.metric("ğŸŒ Top Origin", top.index[0], f"{top.iloc[0]} alerts")
            else:
                st.metric("ğŸŒ Top Origin", "-")
        else:
            st.metric("ğŸŒ Top Origin", "-")

    st.markdown("---")

    # Charts Layout
    c_col1, c_col2 = st.columns(2)
    
    with c_col1:
        st.subheader("êµ­ê°€ë³„ ë°œìƒ í˜„í™© (Top 10)")
        if 'origin_country' in df_filtered.columns:
            top_countries = df_filtered['origin_country'].value_counts().head(10)
            if not top_countries.empty:
                fig = px.bar(
                    x=top_countries.values,
                    y=top_countries.index,
                    orientation='h',
                    labels={'x': 'Count', 'y': 'Country'},
                    color=top_countries.values,
                    color_continuous_scale='Reds'
                )
                fig.update_layout(yaxis={'categoryorder': 'total ascending'})
                st.plotly_chart(fig, width='stretch')
            else:
                st.info("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
    with c_col2:
        st.subheader("ì¼ë³„ ë°œìƒ ì¶”ì´")
        if 'date_parsed' in df_filtered.columns:
            daily_counts = df_filtered.groupby(df_filtered['date_parsed'].dt.date).size().reset_index(name='count')
            if not daily_counts.empty:
                fig2 = px.line(daily_counts, x='date_parsed', y='count', markers=True)
                st.plotly_chart(fig2, width='stretch')
            else:
                 st.info("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # Second row for Hazard Class Distribution
    st.markdown("---")
    st.subheader("ìœ„í•´ìš”ì†Œ ë¶„ë¥˜ ë¶„ì„")
    c_col3, c_col4 = st.columns(2)
    
    with c_col3:
        st.caption("ì‹œí—˜ë¶„ë¥˜ (ëŒ€ë¶„ë¥˜) - Large Class")
        if 'hazard_class_l' in df_filtered.columns:
            hazard_l_dist = df_filtered['hazard_class_l'].value_counts()
            hazard_l_dist = hazard_l_dist[hazard_l_dist.index != ""]
            
            if not hazard_l_dist.empty:
                fig3 = px.pie(
                    names=hazard_l_dist.index,
                    values=hazard_l_dist.values,
                    hole=0.4,
                    color_discrete_sequence=px.colors.qualitative.Set3
                )
                fig3.update_traces(textposition='inside', textinfo='percent+label')
                st.plotly_chart(fig3, width='stretch')
            else:
                st.info("ëŒ€ë¶„ë¥˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.warning("hazard_class_l ì»¬ëŸ¼ ì—†ìŒ")
    
    with c_col4:
        st.caption("ì‹œí—˜ë¶„ë¥˜ (ì¤‘ë¶„ë¥˜) - Middle Class (Top 10)")
        if 'hazard_class_m' in df_filtered.columns:
            hazard_m_dist = df_filtered['hazard_class_m'].value_counts().head(10)
            hazard_m_dist = hazard_m_dist[hazard_m_dist.index != ""]
            
            if not hazard_m_dist.empty:
                fig4 = px.bar(
                    x=hazard_m_dist.values,
                    y=hazard_m_dist.index,
                    orientation='h', # Horizontal bar
                    labels={'x': 'Count', 'y': 'Category'},
                    color=hazard_m_dist.values,
                    color_continuous_scale='Viridis'
                )
                fig4.update_layout(yaxis={'categoryorder': 'total ascending'})
                st.plotly_chart(fig4, width='stretch')
            else:
                st.info("ì¤‘ë¶„ë¥˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
             st.warning("hazard_class_m ì»¬ëŸ¼ ì—†ìŒ")
             
    # Interest Item Analysis (Moved to full width or separate section if needed, keeping simple for now)
    # st.markdown("---")

    # Data Table
    st.markdown("### ğŸ” ìƒì„¸ ë°ì´í„° (Raw Data)")
    
    # Prepare display dataframe with Korean headers
    df_display = df_filtered.drop(columns=['date_parsed'], errors='ignore').copy()
    
    # [UI] Clean up source_detail for display (remove unique ID suffix)
    if 'source_detail' in df_display.columns:
        # Regex: Remove hyphen and following characters if they start with a digit or 'UNKNOWN'
        # This handles 'êµ­ë‚´ì‹í’ˆ ë¶€ì í•©-12345' -> 'êµ­ë‚´ì‹í’ˆ ë¶€ì í•©'
        # But preserves pure names if any
        df_display['source_detail'] = df_display['source_detail'].astype(str).str.replace(r'-(\d+|UNKNOWN).*', '', regex=True)

    df_display = df_display.rename(columns=DISPLAY_HEADERS)
    
    st.dataframe(
        df_display,
        width='stretch',
        hide_index=True
    )
    
    # CSV Download Button
    st.markdown("---")
    col_download1, col_download2 = st.columns([1, 1])
    with col_download1:
        csv_data = df_display.to_csv(index=False).encode('utf-8-sig')
        st.download_button(
            label="ğŸ“¥ ë°ì´í„° ë‹¤ìš´ë¡œë“œ (CSV)",
            data=csv_data,
            file_name=f"food_safety_data_{datetime.now().strftime('%Y%m%d')}.csv",
            mime="text/csv",
            type="primary",
            width='stretch'
        )
    
    with col_download2:
        if st.button("ğŸ”„ ì „ì²´ ë°ì´í„° ì¬ìˆ˜ì§‘", type="secondary", width='stretch'):
            with st.spinner("ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ë° ì „ì²´ ì¬ìˆ˜ì§‘ ì¤‘..."):
                # ê¸°ì¡´ ë°ì´í„° íŒŒì¼ ì‚­ì œ
                hub_file = Path("data/hub/hub_data.parquet")
                if hub_file.exists():
                    hub_file.unlink()
                    st.info("âœ… ê¸°ì¡´ ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
                
                # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰ (once ëª¨ë“œ)
                scheduler = get_scheduler_instance()
                total_count = scheduler.run_all_collectors()
                st.success(f"âœ… ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: {total_count}ê±´ì˜ ìƒˆë¡œìš´ ë°ì´í„°")
                st.cache_data.clear()
                st.rerun()


def main():
    """Main entry point."""
    # Header
    st.markdown('<h1 class="main-header">ğŸ›¡ï¸ Global Food Safety Intelligence Platform</h1>', unsafe_allow_html=True)
    st.markdown("""
    <div style="text-align: center; color: #666; margin-bottom: 20px;">
    ì‹¤ì‹œê°„ ì‹í’ˆ ì•ˆì „ ì •ë³´ í†µí•© ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ (ì‹ì•½ì²˜, FDA, RASFF)
    </div>
    """, unsafe_allow_html=True)

    # Sidebar Navigation
    st.sidebar.title("ğŸ§­ ë©”ë‰´ íƒìƒ‰")
    
    nav_options = {
        "ğŸ“Š í†µí•© ëŒ€ì‹œë³´ë“œ": "Dashboard",
        "ğŸ“š í’ˆëª©ìœ í˜• ê´€ë¦¬": "í’ˆëª©ìœ í˜•",
        "ğŸ“š ì‹œí—˜í•­ëª© ê´€ë¦¬": "ì‹œí—˜í•­ëª©",
        "ğŸ“š ê°œë³„ê¸°ì¤€ê·œê²© ê´€ë¦¬": "ê°œë³„ê¸°ì¤€ê·œê²©",
        "ğŸ“š ê³µí†µê¸°ì¤€ê·œê²© ê´€ë¦¬": "ê³µí†µê¸°ì¤€ê·œê²©",
        "ğŸ“š FDA Import Alert ê´€ë¦¬": "FDA Import Alert ê´€ë¦¬",
        "ğŸ“š FDA í’ˆëª©ìœ í˜• ë§¤í•‘": "FDA í’ˆëª©ìœ í˜• ë§¤í•‘"
    }
    
    selected_nav = st.sidebar.radio(
        "ë°ì´í„° ë° ê¸°ì¤€ì •ë³´ ì„ íƒ",
        list(nav_options.keys())
    )
    
    st.sidebar.markdown("---")
    
    # Sidebar Controls (moved to expander to save space)
    with st.sidebar.expander("ğŸ® ë°ì´í„° ìˆ˜ì§‘ ì»¨íŠ¸ë¡¤", expanded=False):
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ‡°ğŸ‡· MFDS", use_container_width=True):
                with st.spinner("Collecting..."):
                    count = run_collector("MFDS")
                    st.success(f"{count} rec")
                    st.cache_data.clear()
                    st.rerun()
        with col2:
            if st.button("ğŸ‡ºğŸ‡¸ FDA", use_container_width=True):
                with st.spinner("Collecting..."):
                    count = run_collector("FDA", force_update=True)
                    st.success(f"{count} rec")
                    st.cache_data.clear()
                    st.rerun()
                    
        if st.button("ğŸ”„ All Sources Run", type="primary", use_container_width=True):
            with st.spinner("Pipeline..."):
                scheduler = get_scheduler_instance()
                count = scheduler.run_all_collectors()
                st.success(f"Total {count} records")
                st.cache_data.clear()
                st.rerun()

        if st.button("ğŸ—‘ï¸ Clear All", type="secondary", use_container_width=True, help="Delete all collected data, indexes, reports, and reset states."):
            with st.spinner("Clearing all data..."):
                # 1. Clear Hub Data
                hub_path = Path("data/hub/hub_data.parquet")
                if hub_path.exists():
                    try:
                        hub_path.unlink()
                        st.toast("âœ… Hub data deleted.", icon="ğŸ—‘ï¸")
                    except Exception as e:
                        st.error(f"Failed to delete hub data: {e}")

                # 2. Clear FDA Temp Files (Keep Master Index as per user request)
                try:
                    for p_file in Path("data/hub").glob("fda_import_alerts_*.parquet"):
                        p_file.unlink()
                    
                    report_file = Path("reports/fda_collect_summary.md")
                    if report_file.exists():
                        report_file.unlink()
                        st.toast("âœ… FDA Reports cleared.", icon="ğŸ“Š")
                except Exception as e:
                    st.warning(f"Partial error during cleanup: {e}")

                # 3. Clear State Data
                state_path = Path("data/state/fda_counts.json")
                if state_path.exists():
                    try:
                        state_path.unlink()
                    except Exception as e:
                        pass

                # 4. Clear Cache and Rerun
                st.cache_data.clear()
                st.success("All sources and logs have been cleared.")
                st.rerun()

    # Main Router
    page_key = nav_options[selected_nav]
    
    if page_key == "Dashboard":
        df = load_data()
        if df.empty:
            st.warning("âš ï¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ìˆ˜ì§‘ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        else:
            render_dashboard(df)
    else:
        # Master Data Pages (Modularized)
        render_master_data_view(page_key)


    # Footer
    st.markdown("---")
    st.markdown("Â© 2025 CJFSDATACOLLECT Project | Powered by Gemini & Streamlit")

if __name__ == '__main__':
    main()
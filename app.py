"""
Streamlit dashboard for food safety risk analysis.
Provides unified visualization of data from all sources and master data management.
"""

import streamlit as st
import pandas as pd
import plotly.express as px
from pathlib import Path
from datetime import datetime, timedelta
import sys
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Ensure project root is on the Python path
PROJECT_ROOT = Path(__file__).parent
sys.path.insert(0, str(PROJECT_ROOT))

from src.scheduler import DataIngestionScheduler
from src.schema import DISPLAY_HEADERS

# Page configuration
st.set_page_config(
    page_title="Global Food Safety Intelligence",
    page_icon="ğŸ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
    <style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
    div[data-testid="stMetricValue"] {
        font-size: 1.8rem;
    }
    </style>
    """, unsafe_allow_html=True)


@st.cache_data(ttl=300)  # Cache for 5 minutes
def load_data():
    """Load data from hub_data.parquet file."""
    hub_path = Path("data/hub/hub_data.parquet")
    if not hub_path.exists():
        return pd.DataFrame()
    try:
        df = pd.read_parquet(hub_path, engine='pyarrow')
        return df
    except Exception as e:
        st.error(f"Error loading data: {e}")
        return pd.DataFrame()


def get_scheduler_instance():
    """Get a new scheduler instance."""
    return DataIngestionScheduler(data_dir=Path("data/hub"))


def run_collector(collector_name: str, days_back: int = 7):
    """Run a specific collector and return results."""
    scheduler = get_scheduler_instance()
    scheduler.days_back = days_back
    return scheduler.run_single_collector(collector_name)


def render_master_data_tab():
    """Render the Master Data Management tab."""
    st.header("ğŸ“š ê¸°ì¤€ì •ë³´ ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬")
    st.markdown("ì‹ì•½ì²˜ ê¸°ì¤€ì •ë³´(í’ˆëª©ìœ í˜•, ì‹œí—˜í•­ëª© ë“±) Parquet íŒŒì¼ì„ ì¡°íšŒ/ìˆ˜ì •/ì €ì¥í•©ë‹ˆë‹¤.")
    
    REF_DIR = Path("data/reference")
    FILES = {
        "í’ˆëª©ìœ í˜•": "product_code_master.parquet",
        "ì‹œí—˜í•­ëª©": "hazard_code_master.parquet",
        "ê°œë³„ê¸°ì¤€ê·œê²©": "individual_spec_master.parquet",
        "ê³µí†µê¸°ì¤€ê·œê²©": "common_spec_master.parquet"
    }
    
    # 1. File selector
    selected_name = st.selectbox(
        "ğŸ“‚ ê´€ë¦¬í•  ë°±ì„œ ì„ íƒ",
        list(FILES.keys()),
        help="ìˆ˜ì •í•˜ë ¤ëŠ” ê¸°ì¤€ì •ë³´ íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”"
    )
    file_path = REF_DIR / FILES[selected_name]
    
    # 2. Load data
    if not file_path.exists():
        st.error(f"âš ï¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
        st.info("ê¸°ì¤€ì •ë³´ë¥¼ ë¨¼ì € ìƒì„±í•˜ë ¤ë©´ `python src/utils/reference_loader.py`ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return
    
    try:
        # Load full dataset
        df_full = pd.read_parquet(file_path, engine='pyarrow')
        st.success(f"âœ… ë¡œë“œ ì™„ë£Œ: {len(df_full):,}ê±´ì˜ ë ˆì½”ë“œ")
        
        # 3. Search filter
        search_term = st.text_input(
            "ğŸ” ë°ì´í„° ê²€ìƒ‰",
            placeholder="í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì‹í’ˆ, ê²€ì‚¬í•­ëª©ëª… ë“±)",
            help="ëª¨ë“  ì»¬ëŸ¼ì—ì„œ í‚¤ì›Œë“œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤"
        )
        
        # Apply search filter
        if search_term:
            mask = df_full.apply(
                lambda x: x.astype(str).str.contains(search_term, case=False, na=False).any(),
                axis=1
            )
            df_display = df_full[mask].copy()
            st.info(f"ğŸ” ê²€ìƒ‰ ê²°ê³¼: {len(df_display):,}ê±´ (ì „ì²´: {len(df_full):,}ê±´)")
        else:
            df_display = df_full.copy()
        
        # 4. Interactive editor
        st.markdown("---")
        st.subheader("âœï¸ ë°ì´í„° í¸ì§‘ê¸°")
        st.caption("í–‰ ìˆ˜ì •ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. í¸ì§‘ í›„ ë°˜ë“œì‹œ 'ì €ì¥' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
        
        # Apply Korean headers if columns match UNIFIED_SCHEMA
        display_df = df_display.copy()
        display_df = display_df.rename(columns=DISPLAY_HEADERS)
        
        edited_df = st.data_editor(
            display_df,
            num_rows="dynamic",
            use_container_width=True,
            height=500,
            key=f"editor_{selected_name}"
        )
        
        # Convert back to English column names for saving
        reverse_headers = {v: k for k, v in DISPLAY_HEADERS.items()}
        edited_df = edited_df.rename(columns=reverse_headers)
        
        # 5. Save logic
        st.markdown("---")
        col1, col2 = st.columns([3, 1])
        
        with col2:
            if st.button("ğŸ’¾ ë³€ê²½ì‚¬í•­ ì €ì¥", type="primary", use_container_width=True):
                try:
                    if search_term:
                        # í•„í„°ë§ëœ ìƒíƒœì—ì„œëŠ” ì›ë³¸ ë°ì´í„°ì˜ í•´ë‹¹ ì¸ë±ìŠ¤ë§Œ ì—…ë°ì´íŠ¸
                        # combine_firstë‚˜ updateë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘í•©
                        st.info("í•„í„°ë§ëœ ë°ì´í„°ë¥¼ ì›ë³¸ì— ë³‘í•© ì¤‘...")
                        # ì›ë³¸ ë°ì´í„°ì— ìˆ˜ì •ë³¸ ì—…ë°ì´íŠ¸ (ì¸ë±ìŠ¤ ê¸°ì¤€)
                        df_full.update(edited_df)
                        # ì¶”ê°€ëœ í–‰ì´ ìˆë‹¤ë©´ ì²˜ë¦¬ (ì¸ë±ìŠ¤ê°€ ìƒˆë¡œ ìƒì„±ëœ ê²½ìš°)
                        new_rows = edited_df.index.difference(df_full.index)
                        if not new_rows.empty:
                            df_full = pd.concat([df_full, edited_df.loc[new_rows]])
                        
                        save_df = df_full
                    else:
                        save_df = edited_df
                    
                    save_df.to_parquet(file_path, engine='pyarrow', compression='snappy')
                    st.success(f"âœ… {selected_name} ì €ì¥ ì™„ë£Œ!")
                    st.cache_data.clear() # ìºì‹œ ì´ˆê¸°í™”
                    st.rerun()
                except Exception as e:
                    st.error(f"ì €ì¥ ì‹¤íŒ¨: {e}")
                    
    except Exception as e:
        st.error(f"íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


def render_dashboard(df: pd.DataFrame):
    """Render the Main Dashboard tab."""
    # Sidebar filters
    st.sidebar.header("ğŸ“Š Filters")
    
    # Date Range Slider
    if 'registration_date' in df.columns:
        # ë¬¸ìì—´ì„ ë‚ ì§œë¡œ ë³€í™˜
        df['date_parsed'] = pd.to_datetime(df['registration_date'], errors='coerce')
        min_date = df['date_parsed'].min()
        max_date = df['date_parsed'].max()
        
        if pd.notna(min_date) and pd.notna(max_date):
            min_val = min_date.date()
            max_val = max_date.date()
            
            date_range = st.sidebar.slider(
                "Date Range",
                min_value=min_val,
                max_value=max_val,
                value=(min_val, max_val)
            )
        else:
            date_range = None
    else:
        date_range = None
        st.sidebar.warning("ë‚ ì§œ ì»¬ëŸ¼(registration_date)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # Source Filter
    sources = st.sidebar.multiselect(
        "Source",
        options=['FDA', 'RASFF', 'MFDS'],
        default=['FDA', 'RASFF', 'MFDS']
    )
    
    # Hazard Category Filter
    if 'hazard_category' in df.columns:
        available_hazards = sorted(df['hazard_category'].dropna().unique().tolist())
        hazard_categories = st.sidebar.multiselect(
            "Hazard Category",
            options=available_hazards,
            default=available_hazards
        )
    else:
        hazard_categories = None

    # Apply Filters
    df_filtered = df.copy()
    
    if sources:
        df_filtered = df_filtered[df_filtered['data_source'].isin(sources)]
        
    if hazard_categories and 'hazard_category' in df_filtered.columns:
        df_filtered = df_filtered[df_filtered['hazard_category'].isin(hazard_categories)]
        
    if date_range and 'date_parsed' in df_filtered.columns:
        start_date, end_date = date_range
        df_filtered = df_filtered[
            (df_filtered['date_parsed'].dt.date >= start_date) & 
            (df_filtered['date_parsed'].dt.date <= end_date)
        ]

    # Metrics Layout
    st.markdown("### ğŸ“Š Key Metrics")
    m_col1, m_col2, m_col3 = st.columns(3)
    
    with m_col1:
        st.metric("ğŸ“‹ Total Alerts", f"{len(df_filtered):,}")
        
    with m_col2:
        # High Risk Keyword Count
        keywords = ['salmonella', 'listeria', 'e.coli', 'metal', 'glass']
        if 'hazard_item' in df_filtered.columns:
            risk_count = df_filtered['hazard_item'].str.lower().str.contains('|'.join(keywords), na=False).sum()
            st.metric("âš ï¸ Critical Hazards", f"{risk_count:,}")
        else:
            st.metric("âš ï¸ Critical Hazards", "N/A")
            
    with m_col3:
        # Top Country
        if 'origin_country' in df_filtered.columns and not df_filtered.empty:
            top = df_filtered['origin_country'].value_counts().head(1)
            if not top.empty:
                st.metric("ğŸŒ Top Origin", top.index[0], f"{top.iloc[0]} alerts")
            else:
                st.metric("ğŸŒ Top Origin", "-")
        else:
            st.metric("ğŸŒ Top Origin", "-")

    st.markdown("---")

    # Charts Layout
    c_col1, c_col2 = st.columns(2)
    
    with c_col1:
        st.subheader("êµ­ê°€ë³„ ë°œìƒ í˜„í™© (Top 10)")
        if 'origin_country' in df_filtered.columns:
            top_countries = df_filtered['origin_country'].value_counts().head(10)
            fig = px.bar(
                x=top_countries.values,
                y=top_countries.index,
                orientation='h',
                labels={'x': 'Count', 'y': 'Country'},
                color=top_countries.values,
                color_continuous_scale='Reds'
            )
            fig.update_layout(yaxis={'categoryorder': 'total ascending'})
            st.plotly_chart(fig, use_container_width=True)
            
    with c_col2:
        st.subheader("ì¼ë³„ ë°œìƒ ì¶”ì´")
        if 'date_parsed' in df_filtered.columns:
            daily_counts = df_filtered.groupby(df_filtered['date_parsed'].dt.date).size().reset_index(name='count')
            fig2 = px.line(daily_counts, x='date_parsed', y='count', markers=True)
            st.plotly_chart(fig2, use_container_width=True)

    # Data Table
    st.markdown("### ğŸ” ìƒì„¸ ë°ì´í„° (Raw Data)")
    
    # Prepare display dataframe with Korean headers
    df_display = df_filtered.drop(columns=['date_parsed'], errors='ignore').copy()
    df_display = df_display.rename(columns=DISPLAY_HEADERS)
    
    st.dataframe(
        df_display,
        use_container_width=True,
        hide_index=True
    )
    
    # CSV Download Button
    st.markdown("---")
    col_download1, col_download2 = st.columns([1, 3])
    with col_download1:
        csv_data = df_display.to_csv(index=False).encode('utf-8-sig')
        st.download_button(
            label="ğŸ“¥ ë°ì´í„° ë‹¤ìš´ë¡œë“œ (CSV)",
            data=csv_data,
            file_name=f"food_safety_data_{datetime.now().strftime('%Y%m%d')}.csv",
            mime="text/csv",
            type="primary"
        )


def main():
    """Main entry point."""
    # Header
    st.markdown('<h1 class="main-header">ğŸ›¡ï¸ Global Food Safety Intelligence Platform</h1>', unsafe_allow_html=True)
    st.markdown("""
    <div style="text-align: center; color: #666; margin-bottom: 20px;">
    ì‹¤ì‹œê°„ ì‹í’ˆ ì•ˆì „ ì •ë³´ í†µí•© ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ (ì‹ì•½ì²˜, FDA, RASFF)
    </div>
    """, unsafe_allow_html=True)

    # Sidebar Controls
    st.sidebar.header("ğŸ® Data Controls")
    days_back = st.sidebar.number_input("Days to Collect", min_value=1, value=7)
    
    col1, col2 = st.sidebar.columns(2)
    with col1:
        if st.button("ğŸ‡°ğŸ‡· Run MFDS"):
            with st.spinner("Collecting MFDS..."):
                count = run_collector("MFDS", days_back)
                st.success(f"{count} records")
                st.cache_data.clear()
                st.rerun()
    with col2:
        if st.button("ğŸ‡ºğŸ‡¸ Run FDA"):
            with st.spinner("Collecting FDA..."):
                count = run_collector("FDA", days_back)
                st.success(f"{count} records")
                st.cache_data.clear()
                st.rerun()
                
    if st.sidebar.button("ğŸ”„ Run All Sources", type="primary"):
        with st.spinner("Running Full Pipeline..."):
            scheduler = get_scheduler_instance()
            scheduler.days_back = days_back
            count = scheduler.run_all_collectors()
            st.success(f"Total {count} records collected.")
            st.cache_data.clear()
            st.rerun()

    st.sidebar.markdown("---")

    # Tabs
    tab1, tab2 = st.tabs(["ğŸ“Š Dashboard", "ğŸ“š ê¸°ì¤€ì •ë³´ ê´€ë¦¬"])
    
    with tab1:
        df = load_data()
        if df.empty:
            st.warning("âš ï¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ ìˆ˜ì§‘ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        else:
            render_dashboard(df)
            
    with tab2:
        render_master_data_tab()

    # Footer
    st.markdown("---")
    st.markdown("Â© 2025 CJFSDATACOLLECT Project | Powered by Gemini & Streamlit")

if __name__ == '__main__':
    main()
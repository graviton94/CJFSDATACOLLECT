import os
import requests
import pandas as pd
from pathlib import Path
import time
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from loguru import logger

class ReferenceLoader:
    """
    ì‹ì•½ì²˜ ê¸°ì¤€ì •ë³´(ë°±ì„œ)ë¥¼ ìˆ˜ì§‘í•˜ì—¬ Parquet íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ë¡œë” (Zombie Mode ì ìš©)
    
    [ìˆ˜ì§‘ ëŒ€ìƒ]
    1. I2510: ì‹í’ˆê³µì „ í’ˆëª©ìœ í˜•
    2. I2530: ì‹í’ˆê³µì „ ì‹œí—˜í•­ëª©
    3. I2580: ì‹í’ˆê³µì „ ê°œë³„ê¸°ì¤€ê·œê²©
    4. I2600: ì‹í’ˆê³µì „ ê³µí†µê¸°ì¤€ê·œê²©
    """
    
    API_KEY = "4e740c4337844667821c" 
    BASE_URL = "http://openapi.foodsafetykorea.go.kr/api"
    OUTPUT_DIR = Path("data/reference")

    def __init__(self):
        self.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        self.targets = {
            "I2510": {"name": "product_code_master", "desc": "ì‹í’ˆê³µì „ í’ˆëª©ìœ í˜•"},
            "I2530": {"name": "hazard_code_master", "desc": "ì‹í’ˆê³µì „ ì‹œí—˜í•­ëª©"},
            "I2580": {"name": "individual_spec_master", "desc": "ì‹í’ˆê³µì „ ê°œë³„ê¸°ì¤€ê·œê²©"},
            "I2600": {"name": "common_spec_master", "desc": "ì‹í’ˆê³µì „ ê³µí†µê¸°ì¤€ê·œê²©"}
        }
        
        # ì„¸ì…˜ ì„¤ì • (Connection Pool ë° Retry ìë™í™”)
        self.session = requests.Session()
        retries = Retry(total=5, backoff_factor=1, status_forcelist=[500, 502, 503, 504])
        self.session.mount('http://', HTTPAdapter(max_retries=retries))
        self.session.mount('https://', HTTPAdapter(max_retries=retries))

    def fetch_data(self, service_id, target_config):
        """íŠ¹ì • ì„œë¹„ìŠ¤ IDì— ëŒ€í•´ ëê¹Œì§€ Paginationì„ ìˆ˜í–‰ (Retry ê°•í™”)"""
        all_rows = []
        start = 1
        step = 1000
        
        print(f"ğŸ“¥ [{target_config['desc']}] ìˆ˜ì§‘ ì‹œì‘ ({service_id})...")
        
        while True:
            end = start + step - 1
            url = f"{self.BASE_URL}/{self.API_KEY}/{service_id}/json/{start}/{end}"
            
            try:
                # Timeoutì„ 30ì´ˆë¡œ ë„‰ë„‰í•˜ê²Œ ì„¤ì •
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                
                try:
                    data = response.json()
                except ValueError:
                    print(f"   âš ï¸ JSON Decoding Failed at {start}-{end}. Skipping...")
                    break

                # 1. API ì‘ë‹µ êµ¬ì¡° ê²€ì¦
                if service_id not in data:
                    if 'RESULT' in data and 'MSG' in data['RESULT']:
                         msg = data['RESULT']['MSG']
                         # ë°ì´í„° ì—†ìŒ ë©”ì‹œì§€ë©´ ì •ìƒ ì¢…ë£Œ
                         if "í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤" in msg:
                             print(f"   ğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ (End of Data message at {start})")
                             break
                         print(f"   âš ï¸ API Message: {msg}")
                    break

                # 2. ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                if 'row' not in data[service_id]:
                    print(f"   ğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ (No 'row' key at {start})")
                    break
                
                rows = data[service_id]['row']
                if not rows:
                    print(f"   ğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ (Empty rows at {start})")
                    break
                    
                all_rows.extend(rows)
                print(f"   âœ… Fetched {start}~{end} (ëˆ„ì : {len(all_rows)}ê±´)")
                
                start += step
                time.sleep(0.2) # ë”œë ˆì´ ì•½ê°„ ì¦ê°€
                
            except requests.exceptions.ReadTimeout:
                print(f"   â³ Timeout at {start}-{end}. Retrying in 30 seconds...")
                time.sleep(30)
                continue # ì¬ì‹œë„
            except Exception as e:
                print(f"   âŒ Critical Error at {start}-{end}: {e}")
                # ì¹˜ëª…ì  ì˜¤ë¥˜ ì‹œ, ì§€ê¸ˆê¹Œì§€ ëª¨ì€ ê±°ë¼ë„ ì €ì¥í•˜ê¸° ìœ„í•´ ë£¨í”„ ì¢…ë£Œ
                break
        
        # Convert to DataFrame
        df = pd.DataFrame(all_rows)
        
        # Add ANALYZABLE and INTEREST_ITEM columns for hazard_code_master (I2530)
        if service_id == "I2530" and not df.empty:
            if 'ANALYZABLE' not in df.columns:
                df['ANALYZABLE'] = False
            if 'INTEREST_ITEM' not in df.columns:
                df['INTEREST_ITEM'] = False
                
        return df

    def run(self):
        """ì „ì²´ íƒ€ê²Ÿ ì‹¤í–‰ (ìˆ˜ë™ê³ ì • ë°ì´í„° ë³´ì¡´ ë¡œì§ í¬í•¨)"""
        # Primary Key Mapping based on constants
        pk_map = {
            "I2510": "PRDLST_CD",
            "I2530": "TESTITM_CD",
            "I2580": "INDV_SPEC_SEQ",
            "I2600": "CMMN_SPEC_SEQ"
        }

        for service_id, config in self.targets.items():
            try:
                new_df = self.fetch_data(service_id, config)
                if new_df.empty:
                    print(f"âš ï¸ {config['desc']} ìˆ˜ì§‘ ì‹¤íŒ¨: ë°ì´í„° ì—†ìŒ\n")
                    continue
                
                file_path = self.OUTPUT_DIR / f"{config['name']}.parquet"
                pk = pk_map.get(service_id)

                if file_path.exists() and pk:
                    try:
                        old_df = pd.read_parquet(file_path)
                        
                        if 'IS_MANUAL_FIXED' in old_df.columns:
                            # 1. ìˆ˜ë™ ê³ ì •ëœ ë°ì´í„° ì¶”ì¶œ
                            manual_df = old_df[old_df['IS_MANUAL_FIXED'] == True].copy()
                            
                            if not manual_df.empty:
                                print(f"   ğŸ’¡ {len(manual_df)}ê±´ì˜ ìˆ˜ë™ ê³ ì • ë°ì´í„°ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ë³´ì¡´ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
                                
                                # 2. ìƒˆë¡œìš´ ë°ì´í„°ì—ì„œ ìˆ˜ë™ ê³ ì •ëœ PKë¥¼ ì œì™¸
                                if pk in new_df.columns:
                                    manual_pks = manual_df[pk].unique()
                                    new_df = new_df[~new_df[pk].isin(manual_pks)]
                                
                                # 3. ë³‘í•©
                                final_df = pd.concat([manual_df, new_df], ignore_index=True)
                            else:
                                final_df = new_df
                        else:
                            final_df = new_df
                    except Exception as merge_err:
                        print(f"   âš ï¸ ë³‘í•© ì¤‘ ì˜¤ë¥˜ ë°œìƒ, ì‹ ê·œ ë°ì´í„°ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤: {merge_err}")
                        final_df = new_df
                else:
                    final_df = new_df

                # Ensure IS_MANUAL_FIXED exists in final output
                if 'IS_MANUAL_FIXED' not in final_df.columns:
                    final_df['IS_MANUAL_FIXED'] = False

                final_df.to_parquet(file_path, engine='pyarrow', compression='snappy', index=False)
                print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {file_path} (Total {len(final_df)} rows)\n")

            except Exception as e:
                print(f"ğŸš« {config['desc']} ì²˜ë¦¬ ì¤‘ë‹¨: {e}\n")
        
        # 2. Enrich and Standardize Data
        try:
            from src.utils.reference_enricher import ReferenceEnricher
            enricher = ReferenceEnricher()
            enricher.enrich_all()
        except Exception as e:
            print(f"âš ï¸ Enrichment failed: {e}")

        # 3. êµ­ê°€ëª… ë§ˆìŠ¤í„° ë°ì´í„° ì²˜ë¦¬
        self._process_country_master()
    
    def _process_country_master(self):
        """êµ­ê°€ëª… ë§ˆìŠ¤í„° ë°ì´í„° TSV -> Parquet ë³€í™˜"""
        print("ğŸ“¥ êµ­ê°€ëª… ë§ˆìŠ¤í„° ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
        
        tsv_path = self.OUTPUT_DIR / "country_master.tsv"
        parquet_path = self.OUTPUT_DIR / "country_master.parquet"
        
        if not tsv_path.exists():
            print(f"   âš ï¸ {tsv_path} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        try:
            # TSV íŒŒì¼ ì½ê¸° (í•œê¸€ ì¸ì½”ë”© ì§€ì›)
            df = pd.read_csv(tsv_path, sep='\t', encoding='utf-8')
            
            # ì»¬ëŸ¼ëª… ì •ê·œí™” (ë„ì–´ì“°ê¸° ì œê±°, ì˜ë¬¸ìœ¼ë¡œ ë³€í™˜)
            df.columns = [
                'country_name_eng',
                'country_name_kor',
                'iso_2',
                'iso_3',
                'iso_numeric'
            ]
            
            # NULL ê°’ ì²˜ë¦¬
            df = df.fillna('')
            
            # Parquet ì €ì¥
            df.to_parquet(parquet_path, engine='pyarrow', compression='snappy', index=False)
            print(f"   ğŸ’¾ êµ­ê°€ëª… ë§ˆìŠ¤í„° ì €ì¥ ì™„ë£Œ: {parquet_path} (Total {len(df)} rows)")
            
        except Exception as e:
            print(f"   âŒ êµ­ê°€ëª… ë§ˆìŠ¤í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    loader = ReferenceLoader()
    loader.run()
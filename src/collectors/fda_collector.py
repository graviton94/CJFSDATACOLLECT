"""
FDA Import Alert Collector with Context-Aware Block Parsing

This module implements a sophisticated parsing logic that:
1. Starts from the FDA Import Alert publish date index page
2. Extracts detail page links for each Import Alert
3. Parses detail pages using regex-based date detection
4. Anchors data blocks around dates to extract product info, country, and hazard details
5. Maps all data to the unified 14-column schema

Key Features:
- Regex-based date pattern matching (MM/DD/YYYY)
- DOM traversal to find nearest country headers
- Lookup integration for country normalization and hazard classification
- Full schema compliance with validation
"""

import os
import re
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Union
from datetime import datetime

import requests
import pandas as pd
from bs4 import BeautifulSoup, NavigableString, Tag

try:
    from src.schema import UNIFIED_SCHEMA, validate_schema, get_empty_dataframe
    from src.utils.reference_loader import ReferenceLoader
    from src.utils.fuzzy_matcher import FuzzyMatcher
    from src.utils.keyword_mapper import KeywordMapper
except ImportError:
    # Fallback to relative imports
    import sys
    from pathlib import Path
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from schema import UNIFIED_SCHEMA, validate_schema, get_empty_dataframe
    from utils.reference_loader import ReferenceLoader
    from utils.fuzzy_matcher import FuzzyMatcher
    from utils.keyword_mapper import KeywordMapper


class FDACollector:
    """
    US FDA Import Alert collector with precision block parsing.
    
    Workflow:
    1. Fetch index page: https://www.accessdata.fda.gov/cms_ia/iapublishdate.html
    2. Extract Alert Numbers and detail page URLs
    3. For each detail page:
       - Skip the first summary block (Alert #, Published Date, Type)
       - Find all dates using regex (MM/DD/YYYY)
       - For each date:
         * Product Name: Line exactly 1 row above the date
         * Country: Nearest preceding <div class="center"><h4> tag
         * Charge/Reason: Extract associated text block
    4. Normalize data using ReferenceLoader and FuzzyMatcher
    5. Save results to data/hub/ in Parquet format
    """
    
    BASE_URL = "https://www.accessdata.fda.gov/cms_ia"
    INDEX_URL = f"{BASE_URL}/iapublishdate.html"
    OUTPUT_DIR = Path("data/hub")
    REPORT_DIR = Path("reports")
    
    # Default values
    DEFAULT_COUNTRY = "Unknown"
    DEFAULT_ALERT_LIMIT = None  # Process All Alerts
    
    # Regex pattern for MM/DD/YYYY date format (no word boundaries for flexibility)
    DATE_PATTERN = re.compile(r'(\d{2}/\d{2}/\d{4})')
    
    def __init__(self, alert_limit: Union[int, str, None] = None):
        """
        Initialize the collector with necessary directories.
        
        Args:
            alert_limit: Maximum number of alerts to process.
                        - None (default): Processes all alerts (production mode)
                        - int > 0: Custom limit
        """
        self.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        self.REPORT_DIR.mkdir(parents=True, exist_ok=True)
        
        # Set alert processing limit
        self.alert_limit = alert_limit
        
        # Initialize utilities
        self.fuzzy_matcher = FuzzyMatcher()
        self.keyword_mapper = KeywordMapper()
        
        # Load reference data for country normalization
        self._load_reference_data()
    
    def _load_reference_data(self):
        """Load reference data for lookups."""
        reference_dir = Path("data/reference")
        
        # Load country master data if available
        country_file = reference_dir / "country_master.parquet"
        if country_file.exists():
            self.country_ref = pd.read_parquet(country_file)
        else:
            self.country_ref = pd.DataFrame()
        
        # Load hazard master data if available
        hazard_file = reference_dir / "hazard_code_master.parquet"
        if hazard_file.exists():
            self.hazard_ref = pd.read_parquet(hazard_file)
        else:
            self.hazard_ref = pd.DataFrame()
    
    def fetch_index_page(self) -> List[Dict[str, str]]:
        """
        Fetch the Import Alert list.
        Prioritizes the indexed master file (parquet) generated by fda_list_indexer.py.
        Falls back to fetching the web index if parquet is missing.
        
        Returns:
            List of dictionaries containing alert_number, detail_url, and validation metadata
        """
        alerts = []
        
        # 1. Try Loading from Master Index (Parquet)
        master_file = Path("data/reference/fda_list_master.parquet")
        if master_file.exists():
            print(f"üìÑ Loading alerts from master index: {master_file}")
            try:
                df = pd.read_parquet(master_file)
                if not df.empty:
                    # Map columns
                    # Schema: Alert_No, URL, OASIS_Charge_Code_Line, etc.
                    for _, row in df.iterrows():
                        alerts.append({
                            'alert_number': str(row.get('Alert_No', 'Unknown')),
                            'detail_url': row.get('URL', ''),
                            # Pass through metadata for potential future use
                            'last_updated': row.get('Last_Updated_Date', ''),
                            'oasis_code': row.get('OASIS_Charge_Code_Line', ''),
                            'has_red': row.get('Has_Red_List', False),
                            'has_yellow': row.get('Has_Yellow_List', False),
                            'has_green': row.get('Has_Green_List', False),
                            'product_desc_header': row.get('Product_Description', ''),
                            'is_updated': row.get('Is_New_Or_Updated', True),
                            'IsCollect': row.get('IsCollect', True), # Ensure this is passed
                            # Manual Overrides for Hazard/Product/Class
                            'Manual_Hazard_Item': row.get('Manual_Hazard_Item', None),
                            'Manual_Product_Type': row.get('Manual_Product_Type', None),
                            'Manual_Class_M': row.get('Manual_Class_M', None),
                            'Manual_Class_L': row.get('Manual_Class_L', None),
                            'alert_title': row.get('Title', 'Import Alert')
                        })
                    print(f"‚úÖ Loaded {len(alerts)} alerts from local index.")
                    return alerts
            except Exception as e:
                print(f"‚ùå Error reading parquet index: {e}")
        
        # 2. Fallback to Web Scraping (Legacy)
        print("‚ö†Ô∏è Master index not found/readable. Falling back to web scraping...")
        try:
            response = requests.get(self.INDEX_URL, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Find the table containing alert listings
            table = soup.find('table')
            if not table:
                print("‚ö†Ô∏è No table found on index page")
                return alerts
            
            # Extract alert numbers and links
            for row in table.find_all('tr'):
                cols = row.find_all('td')
                if len(cols) < 2:
                    continue
                
                # First column typically contains the alert number with link
                link = cols[0].find('a')
                if not link:
                    continue
                
                alert_number = link.text.strip()
                href = link.get('href', '')
                
                if href:
                    # Construct full URL
                    detail_url = f"{self.BASE_URL}/{href}" if not href.startswith('http') else href
                    alerts.append({
                        'alert_number': alert_number,
                        'detail_url': detail_url
                    })
            
            print(f"‚úÖ Found {len(alerts)} Import Alerts from web")
            
        except Exception as e:
            print(f"‚ùå Error fetching index page: {e}")
        
        return alerts
    
    def _find_nearest_country_header(self, element: Tag) -> Optional[str]:
        """
        Find the nearest preceding <div class="center"><h4> tag for country name.
        
        Args:
            element: The starting element (typically near a date)
            
        Returns:
            Country name as string, or None if not found
        """
        current = element
        
        # Traverse up the DOM tree
        while current:
            # Check previous siblings
            for sibling in current.find_previous_siblings():
                if isinstance(sibling, Tag):
                    # Look for <div class="center"> containing <h4>
                    if sibling.name == 'div' and 'center' in sibling.get('class', []):
                        h4 = sibling.find('h4')
                        if h4:
                            return h4.get_text(strip=True)
                    
                    # Also check nested structures
                    center_div = sibling.find('div', class_='center')
                    if center_div:
                        h4 = center_div.find('h4')
                        if h4:
                            return h4.get_text(strip=True)
            
            # Move to parent
            current = current.parent
            
            # Don't go too far up (e.g., past body tag)
            if current and current.name == 'body':
                break
        
        return None
    
    def _extract_product_and_desc(self, text_lines: List[str], date_index: int) -> Tuple[str, str, str]:
        """
        Extract product name, description, and additional context from text lines.
        
        Args:
            text_lines: List of text lines from the block
            date_index: Index of the line containing the date
            
        Returns:
            Tuple of (product_code_line, description, full_text)
        """
        product_code_line = ""
        description = ""
        full_text_parts = []
        
        # Line above date = Product Name/Code (e.g., "03 R - -..")
        if date_index > 0:
            product_code_line = text_lines[date_index - 1].strip()
        
        # Lines below date = Description
        desc_parts = []
        for i in range(date_index + 1, len(text_lines)):
            line = text_lines[i].strip()
            if line:
                desc_parts.append(line)
        
        description = " ".join(desc_parts)
        
        # Full text includes everything except the date line itself
        for i, line in enumerate(text_lines):
            if i != date_index:  # Skip the date line
                stripped = line.strip()
                if stripped:
                    full_text_parts.append(stripped)
        
        full_text = " ".join(full_text_parts)
        
        return product_code_line, description, full_text
    
    def parse_detail_page(self, alert_meta: Dict) -> List[Dict]:
        """
        Parse a single Import Alert detail page using context-aware block parsing.
        """
        alert_num = alert_meta['alert_number']
        alert_title = alert_meta.get('alert_title', 'Import Alert')
        url = alert_meta['detail_url']
        target_date_str = alert_meta.get('last_updated', '')
        
        print(f"   Parsing Alert {alert_num} (Target Update: {target_date_str})...")
        
        records = []
        
        try:
            response = requests.get(url, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # 1. Identify Country Sections
            country_headers = soup.find_all('div', class_='center')
            
            if not country_headers:
                # Fallback: Process entire text
                self._process_text_block(soup.get_text(), "Unknown", target_date_str, alert_num, alert_title, records, alert_meta)
            else:
                for i, header in enumerate(country_headers):
                    h4 = header.find('h4')
                    if not h4: continue
                    
                    country_name = h4.get_text(strip=True)
                    
                    # Gather text content until next header
                    block_text = ""
                    curr = header.next_sibling
                    
                    while curr:
                        if isinstance(curr, Tag) and curr.name == 'div' and 'center' in curr.get('class', []):
                            break
                        
                        if isinstance(curr, Tag):
                            block_text += curr.get_text(separator='\n') + "\n"
                        elif isinstance(curr, NavigableString):
                            block_text += str(curr) + "\n"
                            
                        curr = curr.next_sibling
                    
                    # Normalize country
                    normalized_country = self._normalize_country_name(country_name)
                    
                    # Process the accumulated text block
                    self._process_text_block(block_text, normalized_country, target_date_str, alert_num, alert_title, records, alert_meta)
            
        except Exception as e:
            print(f"      ‚ùå Error parsing Alert {alert_num}: {e}")
        
        return records

    def _process_text_block(self, text: str, country: str, target_date: str, alert_num: str, alert_title: str, records: List[Dict], overrides: Dict = None):
        """
        Extract records from a country's text block by splitting into Product Blocks.
        Strategy: Find lines starting with Product Code (Digits + Hyphens), 
        treat everything until the next Product Code as one block.
        """
        if not text.strip():
            return
        
        # Prepare overrides if passed as alert_meta (backward compat or simple pass)
        # Actually expected overrides to be passed directly or extracted from alert_meta
        if overrides is None:
            overrides = {}
            
        # Regex to identify a Product Code Line
        # ... (Regex setup) ...
        
        # Regex to identify a Product Code Line
        # Matches: Start of line, Optional whitespace, 2+ digits, anything, hyphen, anything, end of line
        # e.g. "16 M - - 09 Octopus"
        # STRENGTHENED: Exclude lines that look like dates (digits/digits/digits) to prevent false positives
        prod_line_pattern = re.compile(r'(?m)^(\s*\d+[A-Z]?\s*-[^\n]+)$')
        
        # Additional filter: Explicitly ignore lines that are just dates
        # Because sometimes date lines might match the relaxed regex if they contain hyphens instead of slashes
        matches = [
            m for m in prod_line_pattern.finditer(text) 
            if not re.search(r'\d{1,2}[/-]\d{1,2}[/-]\d{4}', m.group(1))
        ]
        
        for i, match in enumerate(matches):
            start_idx = match.start()
            # End index is the start of the next match, or end of text
            end_idx = matches[i+1].start() if i + 1 < len(matches) else len(text)
            
            # Extract the full block for this product
            block_content = text[start_idx:end_idx].strip()
            
            # The first line is the Product Line (from the capture group)
            product_line = match.group(1).strip()
            
            # Process the block content to find Date and Details
            self._parse_single_block(block_content, product_line, country, target_date, alert_num, alert_title, records, overrides)

    def _parse_single_block(self, block_text: str, product_line: str, country: str, target_date: str, alert_num: str, alert_title: str, records: List[Dict], overrides: Dict = None):
        """
        Parse a single isolated product block (Product Line + Date + Details).
        """
        # Look for Date Published within this block
        date_match = re.search(r'Date Published:\s*(\d{1,2}/\d{1,2}/\d{4})', block_text, re.IGNORECASE)
        
        if date_match:
            published_date = date_match.group(1)
            
            # Filter: Check against Target Date
            if target_date and target_date != "Unknown":
                if published_date != target_date:
                    return

            # Extract Product Code (keep first part for Type)
            parts = product_line.split('--', 1)
            if len(parts) < 2:
                parts = product_line.split('-', 1)
            p_code = parts[0].strip()
            
            # USER REQUIREMENT:
            # product_name = Text from start of block UNTIL "Date Published"
            # Remove newlines for a clean single string
            
            # Get text before the date line
            raw_product_text = block_text[:date_match.start()].strip()
            # Replace newlines with spaces and clean up multiple spaces
            clean_product_name = re.sub(r'\s+', ' ', raw_product_text)

            # NEW LOGIC: Extract Hazard Item and Classifications from Block Text
            
            # --- Override & Extraction Logic ---
            extracted_info = {
                "hazard_item": "", 
                "class_m": None, "class_l": None,
                "extraction_source": "None"
            }
            
            # 1. Manual Overrides (Explicit User Setting in Parquet)
            manual_hazard = overrides.get('Manual_Hazard_Item')
            manual_class_m = overrides.get('Manual_Class_M')
            manual_class_l = overrides.get('Manual_Class_L')
            
            if manual_hazard and pd.notna(manual_hazard):
                extracted_info["hazard_item"] = manual_hazard
                extracted_info["class_m"] = manual_class_m if pd.notna(manual_class_m) else None
                extracted_info["class_l"] = manual_class_l if pd.notna(manual_class_l) else None
                extracted_info["extraction_source"] = "Manual_Override"
            
            # 2. Keyword Map (Hard Mapping via Reference)
            if not extracted_info["hazard_item"]:
                keyword_result = self.keyword_mapper.map_hazard(block_text, source='FDA')
                if keyword_result:
                    extracted_info["hazard_item"] = keyword_result['hazard_item']
                    extracted_info["class_m"] = keyword_result['class_m']
                    extracted_info["class_l"] = keyword_result['class_l']
                    extracted_info["extraction_source"] = "Keyword_Map"
                    
            # 3. Fuzzy Matching (Fallback)
            if not extracted_info["hazard_item"]:
                extracted_item = self.fuzzy_matcher.extract_hazard_item_from_text(block_text, self.hazard_ref)
                if extracted_item:
                    extracted_info["hazard_item"] = extracted_item
                    extracted_info["extraction_source"] = "Fuzzy_Fef"

            # --- Classification Logic ---
            # If we have classes (M/L) from override/keyword, use them.
            # Otherwise, lookup using the hazard item.
            hazard_item_value = extracted_info["hazard_item"]
            
            if extracted_info["class_m"] and extracted_info["class_l"]:
                # Fully classified by override/keyword
                hazard_info = {
                    "category": extracted_info["class_m"],
                    "top_category": extracted_info["class_l"],
                    "analyzable": True, # Assume true for manual? Or default?
                    "interest": False
                }
                # Double check if we can get analyzable/interest from lookup anyway if missing?
                # Better to lookup to augment "analyzable/interest" but Keep the M/L classes fixed.
                base_info = self.fuzzy_matcher.match_hazard_category(hazard_item_value, self.hazard_ref)
                hazard_info["analyzable"] = base_info.get("analyzable", True)
                hazard_info["interest"] = base_info.get("interest", False)
                
            else:
                # Need to lookup classification
                hazard_info = self.fuzzy_matcher.match_hazard_category(hazard_item_value, self.hazard_ref)
            
            # --- Product Type ---
            manual_product = overrides.get('Manual_Product_Type')
            final_product_type = p_code
            if manual_product and pd.notna(manual_product):
                final_product_type = manual_product
                
            records.append({
                "registration_date": datetime.strptime(published_date, "%m/%d/%Y").strftime("%Y-%m-%d"),
                "data_source": "FDA",
                "source_detail": f"FDA Import Alert_{alert_num}",
                "product_type": final_product_type,
                "top_level_product_type": None,
                "upper_product_type": None,
                "product_name": clean_product_name, 
                "origin_country": country,
                "notifying_country": "United States",
                "hazard_class_l": hazard_info["top_category"], 
                "hazard_class_m": hazard_info["category"],
                "hazard_item": hazard_item_value, 
                "full_text": block_text, 
                "analyzable": hazard_info.get("analyzable", True), # Default to True
                "interest_item": hazard_info.get("interest", False) 
            })

    def _normalize_country_name(self, raw_country: str) -> str:
        """
        Normalize country name using reference data.
        
        Args:
            raw_country: Raw country name from FDA page
            
        Returns:
            Standardized country name
        """
        if not raw_country or self.country_ref.empty:
            return raw_country
        
        # Try exact match first (case-insensitive)
        raw_lower = raw_country.lower().strip()
        
        for col in ['country_name_eng', 'country_name_kor']:
            if col in self.country_ref.columns:
                mask = self.country_ref[col].astype(str).str.lower().str.strip() == raw_lower
                if mask.any():
                    matched = self.country_ref[mask].iloc[0]
                    return matched.get('country_name_eng', raw_country)
        
        # Return as-is if no match found
        return raw_country

    def collect(self) -> pd.DataFrame:
        """
        Main collection workflow.
        """
        print("üöÄ [FDA] Starting Import Alert collection (User-Defined Logic)...")
        
        # Step 1: Load from Indexer Parquet
        alerts = self.fetch_index_page()
        
        if not alerts:
            print("‚ö†Ô∏è No alerts found from indexer.")
            return get_empty_dataframe()
        
        # Step 2: Parse detail pages
        all_records = []
        
        processed_count = 0
        skipped_green = 0
        skipped_unchanged = 0
        
        for alert_info in alerts:
            # 1. Skip if Unchanged (Differential Crawling)
            if not alert_info.get('is_updated', True):
                skipped_unchanged += 1
                continue
            
            # 1.5. CHECK USER FLAG: IsCollect
            # If IsCollect is explicitly False, skip regardless of other conditions
            if not alert_info.get('IsCollect', True): # Default to True if missing
                print(f"   Skipping Alert {alert_info['alert_number']} (User set IsCollect=False)")
                continue

            # 2. FILTER: Check Lists
            # Logic: If Green exists AND Red/Yellow do NOT exist -> Skip
            # (User: "yellowÎÇò redÏù∏ÏßÄ ÌôïÏù∏ÌïòÍ≥† (greenÏù¥Î©¥ Ïù¥ÌõÑÎèôÏûë Ïä§ÌÇµ)")
            has_red = alert_info.get('has_red', False)
            has_yellow = alert_info.get('has_yellow', False)
            has_green = alert_info.get('has_green', False)
            
            # Interpretation: We need Red or Yellow to proceed. 
            # If ONLY Green (or neither Red/Yellow/Green??), we skip?
            # Let's interpret strict: If NOT (Red or Yellow) -> Skip.
            # This covers "Green Only" and "No List info".
            if not (has_red or has_yellow):
                # print(f"   Skipping Alert {alert_info['alert_number']} (No Red/Yellow List)")
                skipped_green += 1
                continue
            
            # Apply limit if set
            if self.alert_limit and processed_count >= self.alert_limit:
                break

            records = self.parse_detail_page(alert_info)
            if records:
                all_records.extend(records)
            
            processed_count += 1
            if processed_count % 10 == 0:
                print(f"   Progress: {processed_count} updated alerts processed...")
        
        print(f"‚ÑπÔ∏è  Skipped {skipped_unchanged} alerts (No change detected).")
        print(f"‚ÑπÔ∏è  Skipped {skipped_unchanged} alerts (Green List only / No Red-Yellow).")
        
        if not all_records:
            print("‚ö†Ô∏è No new records found matching the update criteria.")
            return get_empty_dataframe()
        
        # Step 3: Convert to DataFrame and validate schema
        df = pd.DataFrame(all_records)
        
        # Remove duplicates immediately to ensure clean data
        initial_count = len(df)
        df = df.drop_duplicates()
        if len(df) < initial_count:
            print(f"   ‚ÑπÔ∏è  Dropped {initial_count - len(df)} duplicate records internally.")
            
        df = validate_schema(df)
        
        print(f"‚úÖ Collected {len(df)} new records from {processed_count} alerts")
        
        # Step 4: Save to Parquet
        output_file = self.OUTPUT_DIR / f"fda_import_alerts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet"
        df.to_parquet(output_file, engine='pyarrow', compression='snappy', index=False)
        print(f"üíæ Saved to {output_file}")
        
        # Step 5: Update summary report
        self._update_summary_report(len(df))
        
        return df
    
    def _update_summary_report(self, record_count: int):
        """
        Update the FDA collection summary report.
        
        Args:
            record_count: Number of new records collected
        """
        report_file = self.REPORT_DIR / "fda_collect_summary.md"
        
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        report_content = f"""# FDA Import Alert Collection Summary

## Latest Collection Run

- **Timestamp**: {timestamp}
- **Records Collected**: {record_count}
- **Data Source**: FDA Import Alerts (iapublishdate.html)
- **Method**: Context-Aware Block Parsing with Regex Date Detection

## Collection Details

The collector implements the following workflow:

1. **Index Page Extraction**: Fetches https://www.accessdata.fda.gov/cms_ia/iapublishdate.html
2. **Alert Discovery**: Extracts Alert Numbers and detail page URLs
3. **Detail Page Parsing**:
   - Skips first summary block (Alert #, Published Date, Type)
   - Uses regex pattern `(\\d{{2}}/\\d{{2}}/\\d{{4}})` to find dates
   - Anchors data extraction around each date:
     * Product Code: Line 1 row above date
     * Description: Lines below date
     * Country: Nearest preceding `<div class="center"><h4>` tag
4. **Data Normalization**:
   - Country names normalized via ReferenceLoader
   - Hazard categories mapped via FuzzyMatcher
5. **Schema Validation**: All records validated against 14-column unified schema

## Schema Compliance

‚úÖ All {record_count} records conform to the 14-column unified schema defined in `src/schema.py`.

## Notes

- Default alert processing limit is {self.DEFAULT_ALERT_LIMIT} for testing
- To process all alerts in production, use: `FDACollector(alert_limit=None)`
- Full text context is preserved in the `full_text` column for future AI-based extraction
"""
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"üìù Updated summary report: {report_file}")


if __name__ == "__main__":
    # Use alert_limit=None for production (processes all alerts)
    # Use alert_limit=5 for testing (processes first 5 alerts)
    collector = FDACollector(alert_limit=5)  # Testing mode
    df = collector.collect()
    print(f"\nüìä Collection complete: {len(df)} records")
    print(df.head())
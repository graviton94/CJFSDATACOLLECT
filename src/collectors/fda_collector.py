"""
FDA Import Alert Collector with Context-Aware Block Parsing

This module implements a sophisticated parsing logic that:
1. Starts from the FDA Import Alert publish date index page
2. Extracts detail page links for each Import Alert
3. Parses detail pages using regex-based date detection
4. Anchors data blocks around dates to extract product info, country, and hazard details
5. Maps all data to the unified 14-column schema

Key Features:
- Regex-based date pattern matching (MM/DD/YYYY)
- DOM traversal to find nearest country headers
- Lookup integration for country normalization and hazard classification
- Full schema compliance with validation
"""

import os
import re
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Union
from datetime import datetime

import requests
import pandas as pd
from bs4 import BeautifulSoup, NavigableString, Tag

try:
    from src.schema import UNIFIED_SCHEMA, validate_schema, get_empty_dataframe
    from src.utils.reference_loader import ReferenceLoader
    from src.utils.fuzzy_matcher import FuzzyMatcher
    from src.utils.keyword_mapper import KeywordMapper
except ImportError:
    # Fallback to relative imports
    import sys
    from pathlib import Path
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from schema import UNIFIED_SCHEMA, validate_schema, get_empty_dataframe
    from utils.reference_loader import ReferenceLoader
    from utils.fuzzy_matcher import FuzzyMatcher
    from utils.keyword_mapper import KeywordMapper


class FDACollector:
    """
    US FDA Import Alert collector with precision block parsing.
    
    Workflow:
    1. Fetch index page: https://www.accessdata.fda.gov/cms_ia/iapublishdate.html
    2. Extract Alert Numbers and detail page URLs
    3. For each detail page:
       - Skip the first summary block (Alert #, Published Date, Type)
       - Find all dates using regex (MM/DD/YYYY)
       - For each date:
         * Product Name: Line exactly 1 row above the date
         * Country: Nearest preceding <div class="center"><h4> tag
         * Charge/Reason: Extract associated text block
    4. Normalize data using ReferenceLoader and FuzzyMatcher
    5. Save results to data/hub/ in Parquet format
    """
    
    BASE_URL = "https://www.accessdata.fda.gov/cms_ia"
    INDEX_URL = f"{BASE_URL}/iapublishdate.html"
    OUTPUT_DIR = Path("data/hub")
    REPORT_DIR = Path("reports")
    
    # Default values
    DEFAULT_COUNTRY = "Unknown"
    DEFAULT_ALERT_LIMIT = None  # Process All Alerts
    
    # Regex pattern for MM/DD/YYYY date format (no word boundaries for flexibility)
    DATE_PATTERN = re.compile(r'(\d{2}/\d{2}/\d{4})')
    
    def __init__(self, alert_limit: Union[int, str, None] = None):
        """
        Initialize the collector with necessary directories.
        
        Args:
            alert_limit: Maximum number of alerts to process.
                        - None (default): Processes all alerts (production mode)
                        - int > 0: Custom limit
        """
        self.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        self.REPORT_DIR.mkdir(parents=True, exist_ok=True)
        
        # Set alert processing limit
        self.alert_limit = alert_limit
        
        # Initialize utilities
        self.fuzzy_matcher = FuzzyMatcher()
        self.keyword_mapper = KeywordMapper()
        
        # Load reference data for country normalization
        self._load_reference_data()
        
        # Load external whitelist mapping
        self.product_whitepaper = self._load_whitepaper()
    
    def _load_reference_data(self):
        """Load reference data for lookups."""
        reference_dir = Path("data/reference")
        
        # Load country master data if available
        country_file = reference_dir / "country_master.parquet"
        if country_file.exists():
            self.country_ref = pd.read_parquet(country_file)
        else:
            self.country_ref = pd.DataFrame()
        
        # Load hazard master data if available
        hazard_file = reference_dir / "hazard_code_master.parquet"
        if hazard_file.exists():
            self.hazard_ref = pd.read_parquet(hazard_file)
        else:
            self.hazard_ref = pd.DataFrame()
    
    def fetch_index_page(self) -> List[Dict[str, str]]:
        """
        Fetch the Import Alert list.
        Prioritizes the indexed master file (parquet) generated by fda_list_indexer.py.
        Falls back to fetching the web index if parquet is missing.
        
        Returns:
            List of dictionaries containing alert_number, detail_url, and validation metadata
        """
        alerts = []
        
        # 1. Try Loading from Master Index (Parquet)
        master_file = Path("data/reference/fda_list_master.parquet")
        if master_file.exists():
            print(f"üìÑ Loading alerts from master index: {master_file}")
            try:
                df = pd.read_parquet(master_file)
                if not df.empty:
                    # Map columns
                    # Schema: Alert_No, URL, OASIS_Charge_Code_Line, etc.
                    for _, row in df.iterrows():
                        alerts.append({
                            'alert_number': str(row.get('Alert_No', 'Unknown')),
                            'detail_url': row.get('URL', ''),
                            # Pass through metadata for potential future use
                            'last_updated': row.get('Last_Updated_Date', ''),
                            'oasis_code': row.get('OASIS_Charge_Code_Line', ''),
                            'has_red': row.get('Has_Red_List', False),
                            'has_yellow': row.get('Has_Yellow_List', False),
                            'has_green': row.get('Has_Green_List', False),
                            'product_desc_header': row.get('Product_Description', ''),
                            'is_updated': row.get('Is_New_Or_Updated', True),
                            'IsCollect': row.get('IsCollect', True), 
                            # Manual Overrides for Hazard/Product/Class
                            'Manual_Hazard_Item': row.get('Manual_Hazard_Item', None),
                            'Manual_Product_Type': row.get('Manual_Product_Type', None),
                            'Manual_Class_M': row.get('Manual_Class_M', None),
                            'Manual_Class_L': row.get('Manual_Class_L', None),
                            'alert_title': row.get('Title', 'Import Alert')
                        })
                    print(f"‚úÖ Loaded {len(alerts)} alerts from local index.")
                    return alerts
            except Exception as e:
                print(f"‚ùå Error reading parquet index: {e}")
        
        # 2. Fallback to Web Scraping (Legacy)
        print("‚ö†Ô∏è Master index not found/readable. Falling back to web scraping...")
        try:
            response = requests.get(self.INDEX_URL, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Find the table containing alert listings
            table = soup.find('table')
            if not table:
                print("‚ö†Ô∏è No table found on index page")
                return alerts
            
            # Extract alert numbers and links
            for row in table.find_all('tr'):
                cols = row.find_all('td')
                if len(cols) < 2:
                    continue
                
                # First column typically contains the alert number with link
                link = cols[0].find('a')
                if not link:
                    continue
                
                alert_number = link.text.strip()
                href = link.get('href', '')
                
                if href:
                    # Construct full URL
                    detail_url = f"{self.BASE_URL}/{href}" if not href.startswith('http') else href
                    alerts.append({
                        'alert_number': alert_number,
                        'detail_url': detail_url
                    })
            
            print(f"‚úÖ Found {len(alerts)} Import Alerts from web")
            
        except Exception as e:
            print(f"‚ùå Error fetching index page: {e}")
        
        return alerts
    
    def _find_nearest_country_header(self, element: Tag) -> Optional[str]:
        """
        Find the nearest preceding <div class="center"><h4> tag for country name.
        
        Args:
            element: The starting element (typically near a date)
            
        Returns:
            Country name as string, or None if not found
        """
        current = element
        
        # Traverse up the DOM tree
        while current:
            # Check previous siblings
            for sibling in current.find_previous_siblings():
                if isinstance(sibling, Tag):
                    # Look for <div class="center"> containing <h4>
                    if sibling.name == 'div' and 'center' in sibling.get('class', []):
                        h4 = sibling.find('h4')
                        if h4:
                            return h4.get_text(strip=True)
                    
                    # Also check nested structures
                    center_div = sibling.find('div', class_='center')
                    if center_div:
                        h4 = center_div.find('h4')
                        if h4:
                            return h4.get_text(strip=True)
            
            # Move to parent
            current = current.parent
            
            # Don't go too far up (e.g., past body tag)
            if current and current.name == 'body':
                break
        
        return None
    
    def _extract_product_and_desc(self, text_lines: List[str], date_index: int) -> Tuple[str, str, str]:
        """
        Extract product name, description, and additional context from text lines.
        
        Args:
            text_lines: List of text lines from the block
            date_index: Index of the line containing the date
            
        Returns:
            Tuple of (product_code_line, description, full_text)
        """
        product_code_line = ""
        description = ""
        full_text_parts = []
        
        # Line above date = Product Name/Code (e.g., "03 R - -..")
        if date_index > 0:
            product_code_line = text_lines[date_index - 1].strip()
        
        # Lines below date = Description
        desc_parts = []
        for i in range(date_index + 1, len(text_lines)):
            line = text_lines[i].strip()
            if line:
                desc_parts.append(line)
        
        description = " ".join(desc_parts)
        
        # Full text includes everything except the date line itself
        for i, line in enumerate(text_lines):
            if i != date_index:  # Skip the date line
                stripped = line.strip()
                if stripped:
                    full_text_parts.append(stripped)
        
        full_text = " ".join(full_text_parts)
        
        return product_code_line, description, full_text
    
    def parse_detail_page(self, alert_meta: Dict, force_update: bool = False) -> List[Dict]:
        """
        State-Machine Parsing Logic v3.2 (Spec Compliance).
        Iterates siblings and maintains a 'current_product' state to handle flat DOM structures.
        """
        alert_num = alert_meta['alert_number']
        alert_title = alert_meta.get('alert_title', 'Import Alert')
        url = alert_meta['detail_url']
        target_date_str = alert_meta.get('last_updated', '')
        
        print(f"   Parsing Alert {alert_num} (Target Update: {target_date_str})...")
        
        records = []
        # Updated Regex: Flexible whitespace and Alphanumeric Subclasses
        # Matches: "16 A - - 04" (Standard), "16 X - T 21" (Subclass T), "16  A  - -  04" (Spaces)
        prod_code_pattern = re.compile(r'(\d{2}\s+[A-Z0-9]\s+[A-Z0-9-]\s+[A-Z0-9-]\s+\d{2})')
        
        try:
            response = requests.get(url, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # 1. Segment by Country Headers
            country_nodes = soup.find_all('div', class_='center')
            if not country_nodes:
                country_nodes = soup.find_all('h4')
            
            # State Buffer
            current_record = None # { 'product_line': str, 'published_date': str, 'raw_content': [], 'country': str }

            for country_node in country_nodes:
                # Resolve Country Name
                h4 = country_node if country_node.name == 'h4' else country_node.find('h4')
                if not h4: continue
                
                raw_country = h4.get_text(strip=True)
                this_country_norm = self._normalize_country_name(raw_country)
                
                # Flush existing record (End of previous country context)
                if current_record:
                    self._flush_buffer_to_record(current_record, records, target_date_str, force_update, alert_num, alert_title, alert_meta)
                    current_record = None
                
                # Iterate Siblings
                curr = country_node.next_sibling
                while curr:
                    if isinstance(curr, Tag):
                        # BREAK Condition: Next Country Header found
                        is_new_country = False
                        if curr.name == 'h4': is_new_country = True
                        if curr.name == 'div' and 'center' in curr.get('class', []):
                             if curr.find('h4'): is_new_country = True
                        
                        if is_new_country:
                            break # End of this country's section
                    
                    node_text = curr.get_text(strip=True) if isinstance(curr, Tag) else str(curr).strip()
                    
                    if not node_text: 
                        curr = curr.next_sibling
                        continue

                    # Condition A: Product Code Trigger -> New Record
                    if prod_code_pattern.search(node_text):
                        # Flush previous record
                        if current_record:
                            self._flush_buffer_to_record(current_record, records, target_date_str, force_update, alert_num, alert_title, alert_meta)
                        
                        # Start New Record
                        current_record = {
                            'product_line': node_text,
                            'published_date': None,
                            'raw_content': [],
                            'country': this_country_norm
                        }
                    
                    # Condition B: Date Published Trigger
                    elif "Date Published:" in node_text:
                        # print(f"      [DEBUG] Found Date Trigger: {node_text}")
                        if current_record:
                            date_match = re.search(r'(\d{1,2}/\d{1,2}/\d{4})', node_text)
                            if date_match:
                                current_record['published_date'] = date_match.group(1)
                    
                    # Condition C: Content Accumulation
                    else:
                        if current_record:
                            current_record['raw_content'].append(node_text)
                    
                    curr = curr.next_sibling
                
                # End of Country Loop -> Flush remaining record for this country
                if current_record:
                    self._flush_buffer_to_record(current_record, records, target_date_str, force_update, alert_num, alert_title, alert_meta)
                    current_record = None

        except Exception as e:
            print(f"      ‚ùå Error parsing Alert {alert_num}: {e}")
            import traceback
            traceback.print_exc()
        
        return records


    def _load_whitepaper(self) -> Dict[str, str]:
        """Load FDA Code to KOR_NM mapping from parquet."""
        path = Path("data/reference/fda_product_code_mapping.parquet")
        if not path.exists():
            return {}
        try:
            df = pd.read_parquet(path)
            return df.set_index('FDA_CODE')['KOR_NM'].to_dict()
        except Exception:
            return {}

    def _flush_buffer_to_record(self, record_state, records_list, target_date, force, alert_num, alert_title, meta):
        """Helper to validate and save the buffered record with robust product mapping."""
        # Must have a date to be valid
        if not record_state.get('published_date'): return
        
        p_date = record_state['published_date'].strip()
        
        # 1. Parse Dates
        try:
            rec_dt = datetime.strptime(p_date, "%m/%d/%Y")
            target_dt = datetime.strptime(target_date, "%m/%d/%Y")
        except:
            # Date parsing fail -> Skip safely
            return

        # 2. Filtering Logic
        if not force:
            if rec_dt != target_dt:
                return

        # 3. Product Type Mapping (Whitepaper Priority)
        product_line = record_state.get('product_line', '').strip()
        industry_code = ""
        
        # Extract Industry Code (First 2 digits of the product code pattern)
        code_match = self.prod_code_pattern.search(product_line)
        if code_match:
            full_code_str = code_match.group(0)
            industry_code = full_code_str.split()[0] if full_code_str else "" 
        
        # Priority 1: Whitepaper from External Parquet
        final_product_type = self.product_whitepaper.get(industry_code)
        
        # Priority 2: Manual Override (from Master Index)
        if not final_product_type:
             manual_type = meta.get('Manual_Product_Type')
             if manual_type and str(manual_type).strip():
                 final_product_type = manual_type

        # Priority 3: Fallback
        if not final_product_type:
            final_product_type = f"Í∏∞ÌÉÄ(FDA Code {industry_code})" if industry_code else "Í∏∞ÌÉÄ"

        # 4. Clean Product Name
        clean_name = product_line
        if code_match:
             clean_name = product_line.replace(full_code_str, "").strip()
        
        # Ensure clean name is not empty
        if len(clean_name) < 2 and record_state['raw_content']:
             clean_name = record_state['raw_content'][0]

        # 5. Build Record
        full_text = "\n".join([product_line] + record_state['raw_content'])
        
        final_record = {
            'alert_number': alert_num,
            'alert_title': alert_title,
            'product_name': clean_name,       
            'product_type': final_product_type,
            'published_date': record_state['published_date'],
            'country': record_state['country'],
            'full_text': full_text,
            'source_type': 'FDA',
            'hazard_item': meta.get('Manual_Hazard_Item') 
        }
        
        records_list.append(final_record)

    def _record_data(self, block_text: str, product_line: str, published_date: str, country: str, alert_num: str, alert_title: str, records: List[Dict], overrides: Dict):
        """Standard processing for identified blocks (v3.0)."""
        # 1. Product Code & Name extraction
        prod_code_pattern = re.compile(r'(\d{2} [A-Z] - - \d{2})')
        code_match = prod_code_pattern.search(product_line)
        p_code = code_match.group(1) if code_match else product_line.split(' ', 1)[0]
        
        # Clean product name: remove code prefix and cleaning leading separators
        if code_match:
            clean_product_name = product_line[code_match.end():].strip()
            clean_product_name = re.sub(r'^[\s\-,]+', '', clean_product_name)
        else:
            clean_product_name = product_line

        # 2. Hazard Extraction
        extracted_info = {
            "hazard_item": "", 
            "class_m": None, "class_l": None,
            "extraction_source": "None"
        }
        
        # A. Manual Overrides
        manual_hazard = overrides.get('Manual_Hazard_Item')
        manual_class_m = overrides.get('Manual_Class_M')
        manual_class_l = overrides.get('Manual_Class_L')
        
        if manual_hazard and pd.notna(manual_hazard):
            extracted_info["hazard_item"] = manual_hazard
            extracted_info["class_m"] = manual_class_m if pd.notna(manual_class_m) else None
            extracted_info["class_l"] = manual_class_l if pd.notna(manual_class_l) else None
            extracted_info["extraction_source"] = "Manual_Override"
        
        # B. Keyword Map
        if not extracted_info["hazard_item"]:
            keyword_result = self.keyword_mapper.map_hazard(block_text, source='FDA')
            if keyword_result:
                extracted_info["hazard_item"] = keyword_result['hazard_item']
                extracted_info["class_m"] = keyword_result['class_m']
                extracted_info["class_l"] = keyword_result['class_l']
                extracted_info["extraction_source"] = "Keyword_Map"
                
        # C. Fuzzy Matching
        if not extracted_info["hazard_item"]:
            extracted_item = self.fuzzy_matcher.extract_hazard_item_from_text(block_text, self.hazard_ref)
            if extracted_item:
                extracted_info["hazard_item"] = extracted_item
                extracted_info["extraction_source"] = "Fuzzy_Ref"

        # Classification Lookup (final refinement)
        hazard_item_value = extracted_info["hazard_item"]
        if extracted_info["class_m"] and extracted_info["class_l"]:
            hazard_info = {
                "category": extracted_info["class_m"],
                "top_category": extracted_info["class_l"],
                "analyzable": True, "interest": False
            }
            # Augment with metadata from lookup
            base_info = self.fuzzy_matcher.match_hazard_category(hazard_item_value, self.hazard_ref)
            hazard_info["analyzable"] = base_info.get("analyzable", True)
            hazard_info["interest"] = base_info.get("interest", False)
        else:
            hazard_info = self.fuzzy_matcher.match_hazard_category(hazard_item_value, self.hazard_ref)
        
        # Product Type Override
        manual_product = overrides.get('Manual_Product_Type')
        final_product_type = p_code
        if manual_product and pd.notna(manual_product):
            final_product_type = manual_product
            
        # 3. Create Record matching UNIFIED_SCHEMA (15 columns)
        records.append({
            "registration_date": datetime.strptime(published_date, "%m/%d/%Y").strftime("%Y-%m-%d"),
            "data_source": "FDA",
            "source_detail": f"FDA Import Alert_{alert_num}",
            "product_type": final_product_type,
            "top_level_product_type": None,
            "upper_product_type": None,
            "product_name": clean_product_name, 
            "origin_country": country,
            "notifying_country": "United States",
            "hazard_class_l": hazard_info["top_category"], 
            "hazard_class_m": hazard_info["category"],
            "hazard_item": hazard_item_value, 
            "full_text": block_text, 
            "analyzable": hazard_info.get("analyzable", True),
            "interest_item": hazard_info.get("interest", False) 
        })

    def _normalize_country_name(self, raw_country: str) -> str:
        """
        Normalize country name using reference data.
        
        Args:
            raw_country: Raw country name from FDA page
            
        Returns:
            Standardized country name
        """
        if not raw_country or self.country_ref.empty:
            return raw_country
        
        # Try exact match first (case-insensitive)
        raw_lower = raw_country.lower().strip()
        
        for col in ['country_name_eng', 'country_name_kor']:
            if col in self.country_ref.columns:
                mask = self.country_ref[col].astype(str).str.lower().str.strip() == raw_lower
                if mask.any():
                    matched = self.country_ref[mask].iloc[0]
                    return matched.get('country_name_eng', raw_country)
        
        # Return as-is if no match found
        return raw_country

    def collect(self, force_update: bool = True) -> pd.DataFrame:
        """
        Main collection workflow.
        
        Args:
            force_update: If True, ignore 'is_updated' flag and collect all target alerts.
        """
        print(f"üöÄ [FDA] Starting Import Alert collection (User-Defined Logic) [Force={force_update}]...")
        
        # Step 1: Load from Indexer Parquet
        alerts = self.fetch_index_page()
        
        if not alerts:
            print("‚ö†Ô∏è No alerts found from indexer.")
            return get_empty_dataframe()
        
        # Step 2: Parse detail pages
        all_records = []
        
        processed_count = 0
        skipped_green = 0
        skipped_unchanged = 0
        
        for alert_info in alerts:
            # 1. Skip if Unchanged (Differential Crawling) - UNLESS Forced
            if not force_update and not alert_info.get('is_updated', True):
                skipped_unchanged += 1
                continue
            
            # 1.5. CHECK USER FLAG: IsCollect
            # If IsCollect is explicitly False, skip regardless of other conditions
            if not alert_info.get('IsCollect', True): # Default to True if missing
                print(f"   Skipping Alert {alert_info['alert_number']} (User set IsCollect=False)")
                continue

            # 2. FILTER: Check Lists
            # Logic: If Green exists AND Red/Yellow do NOT exist -> Skip
            # (User: "yellowÎÇò redÏù∏ÏßÄ ÌôïÏù∏ÌïòÍ≥† (greenÏù¥Î©¥ Ïù¥ÌõÑÎèôÏûë Ïä§ÌÇµ)")
            has_red = alert_info.get('has_red', False)
            has_yellow = alert_info.get('has_yellow', False)
            
            if not (has_red or has_yellow):
                # Only print skip if it wasn't already obvious
                skipped_green += 1
                continue
            
            print(f"   >>> Processing Alert {alert_info['alert_number']} (Red={has_red}, Yellow={has_yellow})")
            
            # Apply limit if set
            if self.alert_limit and processed_count >= self.alert_limit:
                break

            records = self.parse_detail_page(alert_info, force_update=force_update)
            if records:
                print(f"      ‚úÖ Extracted {len(records)} records from Alert {alert_info['alert_number']}")
                all_records.extend(records)
            else:
                print(f"      ‚ö†Ô∏è No records found in Alert {alert_info['alert_number']}")
            
            processed_count += 1
            if processed_count % 10 == 0:
                print(f"   Progress: {processed_count} alerts processed...")
        
        print(f"‚ÑπÔ∏è  Skipped {skipped_unchanged} alerts (No change detected / Not forced).")
        print(f"‚ÑπÔ∏è  Skipped {skipped_green} alerts (Green List only / No Red-Yellow).")
        
        if not all_records:
            print("‚ö†Ô∏è No new records found matching the update criteria.")
            return get_empty_dataframe()
        
        # Step 3: Convert to DataFrame and validate schema
        df = pd.DataFrame(all_records)
        
        # Remove duplicates immediately to ensure clean data
        initial_count = len(df)
        df = df.drop_duplicates()
        if len(df) < initial_count:
            print(f"   ‚ÑπÔ∏è  Dropped {initial_count - len(df)} duplicate records internally.")
            
        df = validate_schema(df)
        
        print(f"‚úÖ Collected {len(df)} new records from {processed_count} alerts")
        
        # Step 4: Save to Parquet
        output_file = self.OUTPUT_DIR / f"fda_import_alerts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet"
        df.to_parquet(output_file, engine='pyarrow', compression='snappy', index=False)
        print(f"üíæ Saved to {output_file}")
        
        # Step 5: Update summary report
        self._update_summary_report(len(df))
        
        return df
    
    def _update_summary_report(self, record_count: int):
        """
        Update the FDA collection summary report.
        
        Args:
            record_count: Number of new records collected
        """
        report_file = self.REPORT_DIR / "fda_collect_summary.md"
        
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        report_content = f"""# FDA Import Alert Collection Summary
 
 ## Latest Collection Run
 
 - **Timestamp**: {timestamp}
 - **Records Collected**: {record_count}
 - **Data Source**: FDA Import Alerts (iapublishdate.html)
 - **Method**: DOM-Centric Segmenting and Anchoring (v3.0)
 
 ## Collection Details
 
 The collector implements the following workflow:
 
 1. **Index Page Extraction**: Fetches https://www.accessdata.fda.gov/cms_ia/iapublishdate.html
 2. **Alert Discovery**: Extracts Alert Numbers and detail page URLs
 3. **DOM-Centric Parsing**:
    - **Segmentation**: Page is divided into country blocks using `div.center > h4`.
    - **Anchoring**: Identifies "Date Published:" nodes as primary anchors.
    - **Backward Trace**: Finds the nearest preceding "Product Line" using regex `(\\d{{2}} [A-Z] - - \\d{{2}})`.
    - **Forward Accumulation**: Gathers all detail nodes (Desc, Notes, Problems) until the next anchor.
 4. **Data Normalization**:
    - Product names are cleaned by removing industry/class codes.
    - Country names normalized via ReferenceLoader.
    - Hazard categories mapped via FuzzyMatcher and KeywordMapper.
 5. **Schema Validation**: All records validated against the 15-column UNIFIED_SCHEMA.
 
 ## Schema Compliance
 
 ‚úÖ All {record_count} records conform to the 15-column unified schema defined in `src/schema.py`.
 
 ## Notes
 
 - Default alert processing limit is {self.DEFAULT_ALERT_LIMIT} for testing.
 - To process all alerts in production, use: `FDACollector(alert_limit=None)`.
 - Full text context is preserved in the `full_text` column for audit and future extraction.
 """
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"üìù Updated summary report: {report_file}")


if __name__ == "__main__":
    # Use alert_limit=None for production (processes all alerts)
    # Use alert_limit=5 for testing (processes first 5 alerts)
    collector = FDACollector(alert_limit=50)  # Testing mode
    df = collector.collect(force_update=True)
    print(f"\nüìä Collection complete: {len(df)} records")
    print(df.head())
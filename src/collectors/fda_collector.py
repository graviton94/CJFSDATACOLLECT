import os
import json
import requests
import pandas as pd
from bs4 import BeautifulSoup
from datetime import datetime
from src.schema import UNIFIED_SCHEMA, validate_schema, get_empty_dataframe

class FDACollector:
    """
    US FDA Import Alert ìˆ˜ì§‘ê¸° (Schema v2 ì ìš©)
    """
    
    BASE_URL = "https://www.accessdata.fda.gov/cms_ia"
    LIST_URL = f"{BASE_URL}/countrylist.html"
    STATE_FILE = "data/state/fda_counts.json"
    
    def __init__(self):
        os.makedirs(os.path.dirname(self.STATE_FILE), exist_ok=True)

    # ... [get_current_counts, load_previous_counts, save_current_countsëŠ” ê¸°ì¡´ ë¡œì§ ìœ ì§€] ...
    # ì§€ë©´ ê´€ê³„ìƒ í•µì‹¬ íŒŒì‹± ë¡œì§ì¸ parse_detail_pageë¥¼ ì¤‘ì ì ìœ¼ë¡œ ìˆ˜ì •í•©ë‹ˆë‹¤.

    def get_current_counts(self):
        # (ê¸°ì¡´ê³¼ ë™ì¼í•˜ë¯€ë¡œ ìƒëµ ê°€ëŠ¥í•˜ì§€ë§Œ, ì‹¤í–‰ì„ ìœ„í•´ ê°„ëµ ë²„ì „ í¬í•¨)
        try:
            response = requests.get(self.LIST_URL, timeout=15)
            soup = BeautifulSoup(response.text, 'html.parser')
            country_map = {}
            for table in soup.find_all('table'):
                for row in table.find_all('tr'):
                    cols = row.find_all('td')
                    if len(cols) < 2: continue
                    link = cols[0].find('a')
                    if not link: continue
                    
                    try: count = int(cols[1].text.strip())
                    except: count = 0
                    
                    href = link.get('href')
                    code = href.split('_')[-1].replace('.html', '')
                    country_map[code] = {"name": link.text.strip(), "url": f"{self.BASE_URL}/{href}", "count": count}
            return country_map
        except: return {}

    def load_previous_counts(self):
        if os.path.exists(self.STATE_FILE):
             try:
                with open(self.STATE_FILE, 'r') as f:
                    return json.load(f)
             except json.JSONDecodeError:
                return {}
             except Exception:
                return {}
        return {}

    def save_current_counts(self, data):
        with open(self.STATE_FILE, 'w') as f:
            json.dump({k: v['count'] for k, v in data.items()}, f)

    def parse_detail_page(self, country_info):
        """ìƒì„¸ í˜ì´ì§€ íŒŒì‹± ë° ì‹ ê·œ ìŠ¤í‚¤ë§ˆ ë§¤í•‘"""
        url = country_info['url']
        country_name = country_info['name']
        print(f"   Make request to -> {country_name}")
        
        results = []
        try:
            response = requests.get(url, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # FDA í…Œì´ë¸” êµ¬ì¡° íŒŒì‹± (ì¼ë°˜ì ì¸ êµ¬ì¡° ê°€ì •)
            for table in soup.find_all('table'):
                rows = table.find_all('tr')
                for row in rows:
                    cols = row.find_all('td')
                    if len(cols) < 3: continue
                    
                    try:
                        # ì˜ˆì‹œ ë§¤í•‘ (ì‹¤ì œ HTML êµ¬ì¡°ì— ë”°ë¼ ì¸ë±ìŠ¤ ì¡°ì • í•„ìš”)
                        # ë³´í†µ: Alert # | Date | Product | Manufacturer | Charge(Reason)
                        alert_num = cols[0].text.strip()
                        pub_date_raw = cols[1].text.strip()
                        product_desc = cols[2].text.strip()
                        reason_desc = cols[-1].text.strip()
                        
                        # ë‚ ì§œ ë³€í™˜ (MM/DD/YYYY -> YYYY-MM-DD)
                        try:
                            dt = datetime.strptime(pub_date_raw, "%m/%d/%Y")
                            reg_date = dt.strftime("%Y-%m-%d")
                        except:
                            reg_date = datetime.now().strftime("%Y-%m-%d")

                        # 13ê°œ ì»¬ëŸ¼ ë§¤í•‘
                        record = {
                            "registration_date": reg_date,
                            "data_source": "FDA",
                            "source_detail": f"Import Alert {alert_num}",
                            "product_type": "Imported Food", # FDAëŠ” ìœ í˜•ì´ ë¹„ì •í˜•ì´ë¼ ê³ ì • í˜¹ì€ íŒŒì‹± í•„ìš”
                            "top_level_product_type": None, # Lookup ì¶”í›„ ì ìš©
                            "upper_product_type": None,
                            "product_name": product_desc,
                            "origin_country": country_name,
                            "notifying_country": "United States",
                            "hazard_category": "Uncategorized", # Reason ë¶„ì„ í•„ìš”
                            "hazard_item": reason_desc,
                            "analyzable": False,
                            "interest_item": False
                        }
                        results.append(record)
                    except: continue
        except Exception as e:
            print(f"   âš ï¸ Error parsing {country_name}: {e}")
        
        return results

    def collect(self):
        print("ğŸš€ [FDA] ìˆ˜ì§‘ ì‹œì‘ (CDC Mode)...")
        curr = self.get_current_counts()
        prev = self.load_previous_counts()
        
        target_countries = []
        for code, info in curr.items():
            if info['count'] > prev.get(code, 0):
                target_countries.append(info)
        
        # ìµœì´ˆ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸ë¡œ 1ê°œ ê°•ì œ ì¶”ê°€
        if not prev and curr and not target_countries:
            target_countries.append(list(curr.values())[0])
            
        all_records = []
        for country in target_countries:
            all_records.extend(self.parse_detail_page(country))
            
        self.save_current_counts(curr)
        
        if not all_records:
            print("âœ… ë³€ê²½ ì‚¬í•­ ì—†ìŒ.")
            return get_empty_dataframe()
            
        df = pd.DataFrame(all_records)
        return validate_schema(df)

if __name__ == "__main__":
    c = FDACollector()
    print(c.collect().head())
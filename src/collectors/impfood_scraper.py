import re
import pandas as pd
from datetime import datetime
from pathlib import Path
from playwright.sync_api import sync_playwright
from src.schema import validate_schema, get_empty_dataframe, UNIFIED_SCHEMA

class ImpFoodScraper:
    """
    ìˆ˜ì…ì‹í’ˆì •ë³´ë§ˆë£¨ (Imported Food Safety Portal) ìŠ¤í¬ë˜í¼
    - ëŒ€ìƒ: ìˆ˜ì…ì‹í’ˆ ë¶€ì í•© ì •ë³´
    - ë°©ì‹: Playwrightë¥¼ ì´ìš©í•œ DOM ì†ì„± ì¶”ì¶œ
    """
    
    # ë² ì´ìŠ¤ URL (ê²€ìƒ‰ ì¡°ê±´: ì „ì²´ ì¡°íšŒ, 100ê°œì”© ë³´ê¸°)
    BASE_URL = "https://impfood.mfds.go.kr/CFCEE01F01/getList?limit=100&searchCondition=pdNm"
    REF_DIR = Path("data/reference")
    
    def __init__(self):
        # Load reference data as DataFrame (consistent with MFDS approach)
        print("ğŸ“¥ ê¸°ì¤€ì •ë³´(Reference Data) ë¡œë“œ ì¤‘...")
        self.product_ref_df = self._load_reference_df("product_code_master.parquet")
        self.hazard_ref_df = self._load_reference_df("hazard_code_master.parquet")
        print("âœ… ê¸°ì¤€ì •ë³´ ë¡œë“œ ì™„ë£Œ.")

    def _load_reference_df(self, filename):
        """
        Parquet íŒŒì¼ì„ DataFrameìœ¼ë¡œ ë¡œë“œ (Multi-column ê²€ìƒ‰ ì§€ì›)
        - Consistent with MFDSCollector approach
        """
        file_path = self.REF_DIR / filename
        if not file_path.exists():
            print(f"   âš ï¸ Warning: {filename} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. Lookup ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
            return pd.DataFrame()
        
        try:
            df = pd.read_parquet(file_path)
            print(f"   ğŸ“š {filename} ë¡œë“œ ì™„ë£Œ (ì´ {len(df)}ê±´, ì»¬ëŸ¼: {df.columns.tolist()})")
            return df
        except Exception as e:
            print(f"   âŒ {filename} ë¡œë“œ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    def _lookup_product_info(self, product_type):
        """
        í’ˆëª©ìœ í˜• ì´ë¦„ìœ¼ë¡œ ìƒìœ„/ìµœìƒìœ„ ìœ í˜• ì¡°íšŒ
        - Same logic as MFDSCollector._lookup_product_info
        - Returns NAMES (NM) instead of CODES (CD)
        """
        info = {"top": None, "upper": None}
        
        if self.product_ref_df.empty or not product_type:
            return info
        
        # ë§¤ì¹­í•  ì»¬ëŸ¼ë“¤ (KOR_NM, ENG_NM)
        match_columns = ['KOR_NM', 'ENG_NM']
        
        # Normalize search term (strip whitespace to fix matching issues)
        search_term = str(product_type).strip().lower()
        
        # ê° ì»¬ëŸ¼ì—ì„œ ë§¤ì¹­ ì‹œë„ (early exit on first match)
        matched_row = None
        for col in match_columns:
            if col in self.product_ref_df.columns:
                # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í–‰ ì°¾ê¸° (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´, ê³µë°± ì œê±°)
                mask = self.product_ref_df[col].astype(str).str.strip().str.lower() == search_term
                if mask.any():
                    matched_row = self.product_ref_df[mask].iloc[0]
                    break  # Early exit on first match
        
        if matched_row is not None:
            # ì¶œë ¥ í•„ë“œ ì¶”ì¶œ: NAMES instead of CODES
            # Try HTRK_PRDLST_NM first, fallback to GR_NM
            if "HTRK_PRDLST_NM" in matched_row.index and pd.notna(matched_row.get("HTRK_PRDLST_NM")):
                info["top"] = matched_row.get("HTRK_PRDLST_NM")
            elif "GR_NM" in matched_row.index and pd.notna(matched_row.get("GR_NM")):
                info["top"] = matched_row.get("GR_NM")
            
            # Try HRRK_PRDLST_NM first, fallback to PRDLST_CL_NM
            if "HRRK_PRDLST_NM" in matched_row.index and pd.notna(matched_row.get("HRRK_PRDLST_NM")):
                info["upper"] = matched_row.get("HRRK_PRDLST_NM")
            elif "PRDLST_CL_NM" in matched_row.index and pd.notna(matched_row.get("PRDLST_CL_NM")):
                info["upper"] = matched_row.get("PRDLST_CL_NM")
        
        return info

    def _lookup_hazard_info(self, hazard_item):
        """
        ì‹œí—˜í•­ëª© ì´ë¦„ìœ¼ë¡œ ë¶„ë¥˜(ì¹´í…Œê³ ë¦¬) ì¡°íšŒ
        - Same logic as MFDSCollector._lookup_hazard_info
        """
        info = {"category": None, "analyzable": False, "interest": False}
        
        if self.hazard_ref_df.empty or not hazard_item:
            return info
        
        # ë§¤ì¹­í•  ì»¬ëŸ¼ë“¤
        match_columns = ['KOR_NM', 'ENG_NM', 'ABRV', 'NCKNM', 'TESTITM_NM']
        
        # Normalize search term
        search_term = str(hazard_item).strip().lower()
        
        # ê° ì»¬ëŸ¼ì—ì„œ ë§¤ì¹­ ì‹œë„ (early exit on first match)
        matched_row = None
        for col in match_columns:
            if col in self.hazard_ref_df.columns:
                # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í–‰ ì°¾ê¸° (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)
                mask = self.hazard_ref_df[col].astype(str).str.strip().str.lower() == search_term
                if mask.any():
                    matched_row = self.hazard_ref_df[mask].iloc[0]
                    break  # Early exit on first match
        
        if matched_row is not None:
            # ì¶œë ¥ í•„ë“œ ì¶”ì¶œ: M_KOR_NM, ANALYZABLE, INTEREST_ITEM
            info["category"] = matched_row.get("M_KOR_NM") if "M_KOR_NM" in matched_row.index else None
            info["analyzable"] = bool(matched_row.get("ANALYZABLE", False)) if "ANALYZABLE" in matched_row.index else False
            info["interest"] = bool(matched_row.get("INTEREST_ITEM", False)) if "INTEREST_ITEM" in matched_row.index else False
        
        return info

    def scrape(self, max_pages=3):
        """
        ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰
        :param max_pages: ìˆ˜ì§‘í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ 3í˜ì´ì§€, ì•½ 300ê±´)
        """
        print(f"ğŸš€ [ImpFood] ìˆ˜ì…ì‹í’ˆì •ë³´ë§ˆë£¨ ìˆ˜ì§‘ ì‹œì‘ (Max {max_pages} pages)...")
        records = []
        
        try:
            with sync_playwright() as p:
                # ë¸Œë¼ìš°ì € ì‹¤í–‰ (Headless)
                browser = p.chromium.launch(headless=True)
                page = browser.new_page()
                
                # Performance optimization: Block images and fonts
                def block_resources(route):
                    """Block images, fonts, and other unnecessary resources"""
                    if route.request.resource_type in ["image", "font", "stylesheet"]:
                        route.abort()
                    else:
                        route.continue_()
                
                page.route("**/*", block_resources)
                
                for current_page in range(1, max_pages + 1):
                    target_url = f"{self.BASE_URL}&page={current_page}"
                    print(f"   Reading Page {current_page}...")
                    
                    page.goto(target_url, timeout=60000)
                    
                    # ë¦¬ìŠ¤íŠ¸ ë¡œë”© ëŒ€ê¸° (ì²« ë²ˆì§¸ ì¹´ë“œì˜ ì œí’ˆëª…ì´ ëœ° ë•Œê¹Œì§€)
                    try:
                        page.wait_for_selector("div.gallery.type2 ul li h4", timeout=10000)
                    except:
                        print(f"   âš ï¸ Page {current_page}: ë°ì´í„° ë¡œë”© ì‹¤íŒ¨ ë˜ëŠ” ë.")
                        break
                    
                    # ì¹´ë“œ ë¦¬ìŠ¤íŠ¸ í™•ë³´
                    cards = page.locator("div.gallery.type2 ul li").all()
                    print(f"   - Found {len(cards)} items.")
                    
                    if not cards:
                        break
                        
                    for card in cards:
                        try:
                            # 1. ID ì¶”ì¶œ (ìë°”ìŠ¤í¬ë¦½íŠ¸ ë§í¬ì˜ ID ì†ì„±)
                            # ì˜ˆ: <a href="javascript:fnShowVioltCtnt..." id="202600014999">
                            # Locatorë¥¼ ì‚¬ìš©í•˜ì—¬ í•´ë‹¹ a íƒœê·¸ë¥¼ ì°¾ìŒ
                            id_locator = card.locator("a[href^='javascript:fnShowVioltCtnt']")
                            if id_locator.count() == 0:
                                continue
                            unique_id = id_locator.first.get_attribute("id")
                            
                            # 2. ë‚ ì§œ ì¶”ì¶œ
                            date_str = card.locator("span[title='ë¶€ì í•©ì¼ì']").inner_text().strip()
                            
                            # 3. ì œí’ˆëª… ì¶”ì¶œ (í•œê¸€ + ì˜ë¬¸)
                            name_kr = card.locator("strong[title='ì œí’ˆí•œê¸€ëª…']").inner_text().strip()
                            name_en = card.locator("span[title='ì œí’ˆì˜ë¬¸ëª…']").inner_text().strip()
                            product_name = f"{name_kr} ({name_en})" if name_en else name_kr
                            
                            # 4. í’ˆëª© ë° ì›ì‚°ì§€
                            product_type = card.locator("span[title='í’ˆëª©ëª…']").inner_text().strip()
                            origin = card.locator("span[title='ì œì¡°êµ­ê°€']").inner_text().strip()
                            
                            # 5. ìœ„ë°˜ë‚´ì—­ (ì¤‘ìš”: title ì†ì„±ì—ì„œ ê°€ì ¸ì˜´)
                            # a íƒœê·¸ ì•ˆì— ìˆëŠ” spanì˜ title ì†ì„±
                            hazard_item = id_locator.locator("span").get_attribute("title")
                            
                            # Lookup (using updated logic that returns Names)
                            prod_info = self._lookup_product_info(product_type)
                            hazard_info = self._lookup_hazard_info(hazard_item)

                            # 6. ìŠ¤í‚¤ë§ˆ ë§¤í•‘ (13 Columns)
                            record = {
                                "registration_date": date_str, # í¬ë§·ì´ ì´ë¯¸ YYYY-MM-DD í˜•íƒœì„
                                "data_source": "ImpFood",
                                "source_detail": f"ImpFood-{unique_id}",
                                "product_type": product_type,
                                "top_level_product_type": prod_info["top"],
                                "upper_product_type": prod_info["upper"],
                                "product_name": product_name,
                                "origin_country": origin,
                                "notifying_country": "South Korea",
                                "hazard_category": hazard_info["category"],
                                "hazard_item": hazard_item,
                                "analyzable": hazard_info["analyzable"],
                                "interest_item": hazard_info["interest"]
                            }
                            records.append(record)
                            
                        except Exception as e:
                            # ê°œë³„ ì¹´ë“œ íŒŒì‹± ì—ëŸ¬ ì‹œ ìŠ¤í‚µí•˜ê³  ê³„ì† ì§„í–‰
                            # print(f"Card Error: {e}")
                            continue
                            
                browser.close()
                
        except Exception as e:
            print(f"âŒ [ImpFood] Critical Error: {e}")
            return get_empty_dataframe()

        if not records:
            print("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return get_empty_dataframe()
            
        df = pd.DataFrame(records)
        print(f"âœ… [ImpFood] ì´ {len(df)} ê±´ ìˆ˜ì§‘ ë° ì •ê·œí™” ì™„ë£Œ.")
        return validate_schema(df)

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    scraper = ImpFoodScraper()
    df = scraper.scrape(max_pages=1)
    print(df.head())
    print(f"Total collected: {len(df)}")
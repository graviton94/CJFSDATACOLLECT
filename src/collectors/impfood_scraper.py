import re
import pandas as pd
from datetime import datetime
from pathlib import Path
from playwright.sync_api import sync_playwright
from src.schema import validate_schema, get_empty_dataframe, UNIFIED_SCHEMA

class ImpFoodScraper:
    """
    ìˆ˜ì…ì‹í’ˆì •ë³´ë§ˆë£¨ (Imported Food Safety Portal) ìŠ¤í¬ë˜í¼
    - ëŒ€ìƒ: ìˆ˜ì…ì‹í’ˆ ë¶€ì í•© ì •ë³´
    - ë°©ì‹: Playwrightë¥¼ ì´ìš©í•œ DOM ì†ì„± ì¶”ì¶œ
    """
    
    # ë² ì´ìŠ¤ URL (ê²€ìƒ‰ ì¡°ê±´: ì „ì²´ ì¡°íšŒ, 100ê°œì”© ë³´ê¸°)
    BASE_URL = "https://impfood.mfds.go.kr/CFCEE01F01/getList?limit=100&searchCondition=pdNm"
    REF_DIR = Path("data/reference")
    
    def __init__(self):
        # ê¸°ì¤€ì •ë³´ ë¡œë“œ (Lookupìš©)
        self.product_ref = self._load_reference("product_code_master.parquet", "PRDLST_NM")
        self.hazard_ref = self._load_reference("hazard_code_master.parquet", "TEST_ITM_NM")

    def _load_reference(self, filename, index_col):
        """Parquet íŒŒì¼ì„ ì½ì–´ ê²€ìƒ‰ìš© Dictë¡œ ë³€í™˜"""
        file_path = self.REF_DIR / filename
        if not file_path.exists():
            return {}
        try:
            df = pd.read_parquet(file_path)
            if index_col not in df.columns: return {}
            return df.drop_duplicates(subset=[index_col]).set_index(index_col).to_dict('index')
        except: return {}

    def _lookup_info(self, product_type, hazard_item):
        """ê¸°ì¤€ì •ë³´ ë§¤í•‘ ë¡œì§"""
        p_info = {"top": None, "upper": None}
        if product_type in self.product_ref:
            row = self.product_ref[product_type]
            p_info["top"] = row.get("GR_NM") or row.get("HRNK_PRDLST_NM")
            p_info["upper"] = row.get("PRDLST_CL_NM")
            
        h_info = {"cat": "Uncategorized", "analyzable": False, "interest": False}
        # ì‹œí—˜í•­ëª©ì€ ì •í™•íˆ ì¼ì¹˜í•˜ì§€ ì•Šì„ ìˆ˜ ìˆì–´ í¬í•¨ ì—¬ë¶€ë¡œ ì²´í¬ (ê°„ì´ ë¡œì§)
        # ì‹¤ë¬´ì—ì„  Fuzzy Matching í•„ìš”í•˜ì§€ë§Œ, ì—¬ê¸°ì„  Exact Match ìš°ì„ 
        if hazard_item in self.hazard_ref:
            row = self.hazard_ref[hazard_item]
            h_info["cat"] = row.get("LCLS_NM", "Uncategorized")
            # h_info["analyzable"] = ... (ë°±ì„œ ì»¬ëŸ¼ì— ë”°ë¼)
            
        return p_info, h_info

    def scrape(self, max_pages=3):
        """
        ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰
        :param max_pages: ìˆ˜ì§‘í•  ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (ê¸°ë³¸ 3í˜ì´ì§€, ì•½ 300ê±´)
        """
        print(f"ğŸš€ [ImpFood] ìˆ˜ì…ì‹í’ˆì •ë³´ë§ˆë£¨ ìˆ˜ì§‘ ì‹œì‘ (Max {max_pages} pages)...")
        records = []
        
        try:
            with sync_playwright() as p:
                # ë¸Œë¼ìš°ì € ì‹¤í–‰ (Headless)
                browser = p.chromium.launch(headless=True)
                page = browser.new_page()
                
                for current_page in range(1, max_pages + 1):
                    target_url = f"{self.BASE_URL}&page={current_page}"
                    print(f"   Reading Page {current_page}...")
                    
                    page.goto(target_url, timeout=60000)
                    
                    # ë¦¬ìŠ¤íŠ¸ ë¡œë”© ëŒ€ê¸° (ì²« ë²ˆì§¸ ì¹´ë“œì˜ ì œí’ˆëª…ì´ ëœ° ë•Œê¹Œì§€)
                    try:
                        page.wait_for_selector("div.gallery.type2 ul li h4", timeout=10000)
                    except:
                        print(f"   âš ï¸ Page {current_page}: ë°ì´í„° ë¡œë”© ì‹¤íŒ¨ ë˜ëŠ” ë.")
                        break
                    
                    # ì¹´ë“œ ë¦¬ìŠ¤íŠ¸ í™•ë³´
                    cards = page.locator("div.gallery.type2 ul li").all()
                    print(f"   - Found {len(cards)} items.")
                    
                    if not cards:
                        break
                        
                    for card in cards:
                        try:
                            # 1. ID ì¶”ì¶œ (ìë°”ìŠ¤í¬ë¦½íŠ¸ ë§í¬ì˜ ID ì†ì„±)
                            # ì˜ˆ: <a href="javascript:fnShowVioltCtnt..." id="202600014999">
                            # Locatorë¥¼ ì‚¬ìš©í•˜ì—¬ í•´ë‹¹ a íƒœê·¸ë¥¼ ì°¾ìŒ
                            id_locator = card.locator("a[href^='javascript:fnShowVioltCtnt']")
                            if id_locator.count() == 0:
                                continue
                            unique_id = id_locator.first.get_attribute("id")
                            
                            # 2. ë‚ ì§œ ì¶”ì¶œ
                            date_str = card.locator("span[title='ë¶€ì í•©ì¼ì']").inner_text().strip()
                            
                            # 3. ì œí’ˆëª… ì¶”ì¶œ (í•œê¸€ + ì˜ë¬¸)
                            name_kr = card.locator("strong[title='ì œí’ˆí•œê¸€ëª…']").inner_text().strip()
                            name_en = card.locator("span[title='ì œí’ˆì˜ë¬¸ëª…']").inner_text().strip()
                            product_name = f"{name_kr} ({name_en})" if name_en else name_kr
                            
                            # 4. í’ˆëª© ë° ì›ì‚°ì§€
                            product_type = card.locator("span[title='í’ˆëª©ëª…']").inner_text().strip()
                            origin = card.locator("span[title='ì œì¡°êµ­ê°€']").inner_text().strip()
                            
                            # 5. ìœ„ë°˜ë‚´ì—­ (ì¤‘ìš”: title ì†ì„±ì—ì„œ ê°€ì ¸ì˜´)
                            # a íƒœê·¸ ì•ˆì— ìˆëŠ” spanì˜ title ì†ì„±
                            hazard_item = id_locator.locator("span").get_attribute("title")
                            
                            # Lookup
                            p_info, h_info = self._lookup_info(product_type, hazard_item)

                            # 6. ìŠ¤í‚¤ë§ˆ ë§¤í•‘ (13 Columns)
                            record = {
                                "registration_date": date_str, # í¬ë§·ì´ ì´ë¯¸ YYYY-MM-DD í˜•íƒœì„
                                "data_source": "ImpFood",
                                "source_detail": f"ImpFood-{unique_id}",
                                "product_type": product_type,
                                "top_level_product_type": p_info["top"],
                                "upper_product_type": p_info["upper"],
                                "product_name": product_name,
                                "origin_country": origin,
                                "notifying_country": "South Korea",
                                "hazard_category": h_info["cat"],
                                "hazard_item": hazard_item,
                                "analyzable": h_info["analyzable"],
                                "interest_item": h_info["interest"]
                            }
                            records.append(record)
                            
                        except Exception as e:
                            # ê°œë³„ ì¹´ë“œ íŒŒì‹± ì—ëŸ¬ ì‹œ ìŠ¤í‚µí•˜ê³  ê³„ì† ì§„í–‰
                            # print(f"Card Error: {e}")
                            continue
                            
                browser.close()
                
        except Exception as e:
            print(f"âŒ [ImpFood] Critical Error: {e}")
            return get_empty_dataframe()

        if not records:
            print("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return get_empty_dataframe()
            
        df = pd.DataFrame(records)
        return validate_schema(df)

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    scraper = ImpFoodScraper()
    df = scraper.scrape(max_pages=1)
    print(df.head())
    print(f"Total collected: {len(df)}")

import os
import json
import re
import requests
import pandas as pd
from datetime import datetime
from dotenv import load_dotenv
from pathlib import Path

# í†µí•© ìŠ¤í‚¤ë§ˆ ë° ìœ í‹¸ë¦¬í‹° ê°€ì ¸ì˜¤ê¸°
from src.schema import UNIFIED_SCHEMA, validate_schema, generate_record_id, get_empty_dataframe

load_dotenv()

class MFDSCollector:
    """
    ëŒ€í•œë¯¼êµ­ ì‹ì•½ì²˜(MFDS) ìœ„í•´ì •ë³´ ìˆ˜ì§‘ê¸°
    í˜„ì¬ êµ¬í˜„ëœ ì„œë¹„ìŠ¤:
    - I2620: êµ­ë‚´ì‹í’ˆ ë¶€ì í•© ì •ë³´ (Domestic Food Inspection Failure)
    """
    
    BASE_URL = "http://openapi.foodsafetykorea.go.kr/api"
    REF_DIR = Path("data/reference")
    
    def __init__(self):
        self.api_key = os.getenv("KOREA_FOOD_API_KEY")
        if not self.api_key:
            raise ValueError("âŒ Error: KOREA_FOOD_API_KEYê°€ .env íŒŒì¼ì— ì—†ìŠµë‹ˆë‹¤.")
            
        # ---------------------------------------------------------
        # [Smart Lookup] ê¸°ì¤€ì •ë³´ ë¡œë“œ (ë©”ëª¨ë¦¬ ìºì‹±)
        # ---------------------------------------------------------
        print("ğŸ“¥ ê¸°ì¤€ì •ë³´(Reference Data) ë¡œë“œ ì¤‘...")
        self.product_ref_df = self._load_reference_df("product_code_master.parquet")
        self.hazard_ref_df = self._load_reference_df("hazard_code_master.parquet")
        self.country_ref = self._load_country_reference()
        print("âœ… ê¸°ì¤€ì •ë³´ ë¡œë“œ ì™„ë£Œ.")

    def _load_reference_df(self, filename):
        """
        Parquet íŒŒì¼ì„ DataFrameìœ¼ë¡œ ë¡œë“œ (Multi-column ê²€ìƒ‰ ì§€ì›)
        """
        file_path = self.REF_DIR / filename
        if not file_path.exists():
            print(f"   âš ï¸ Warning: {filename} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. Lookup ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
            return pd.DataFrame()
        
        try:
            df = pd.read_parquet(file_path)
            print(f"   ğŸ“š {filename} ë¡œë“œ ì™„ë£Œ (ì´ {len(df)}ê±´, ì»¬ëŸ¼: {df.columns.tolist()})")
            return df
        except Exception as e:
            print(f"   âŒ {filename} ë¡œë“œ ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    def fetch_service(self, service_id, start_idx, end_idx):
        """API í˜¸ì¶œ ë° JSON ì‘ë‹µ ë°˜í™˜"""
        url = f"{self.BASE_URL}/{self.api_key}/{service_id}/json/{start_idx}/{end_idx}"
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            # ë°ì´í„° ê²€ì¦
            if service_id in data and 'row' in data[service_id]:
                return data[service_id]['row']
            # ì—ëŸ¬ ë©”ì‹œì§€ í™•ì¸
            if 'RESULT' in data and 'MSG' in data['RESULT']:
                if "í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤" in data['RESULT']['MSG']:
                    return []
            return []
        except Exception as e:
            print(f"   âš ï¸ API í˜¸ì¶œ ì—ëŸ¬ ({start_idx}-{end_idx}): {e}")
            return []

    def normalize_date(self, date_str):
        """ë‚ ì§œ ë³€í™˜: 2025.03.12 -> 2025-03-12"""
        if not date_str: return None
        return date_str.replace('.', '-')

    def _lookup_product_info(self, product_type):
        """
        í’ˆëª©ìœ í˜• ì´ë¦„ìœ¼ë¡œ ìƒìœ„/ìµœìƒìœ„ ìœ í˜• ì¡°íšŒ
        
        Logic 1: Product Hierarchy Lookup
        - Input: product_type (from API)
        - Reference: product_code_master.parquet
        - Matching Rule: Find row where product_type matches KOR_NM OR ENG_NM
        - Output Mapping:
          - top_level_product_type â† HTRK_PRDLST_CD (from reference)
          - upper_product_type â† HRRK_PRDLST_CD (from reference)
        """
        info = {"top": None, "upper": None}
        
        if self.product_ref_df.empty or not product_type:
            return info
        
        # ë§¤ì¹­í•  ì»¬ëŸ¼ë“¤ (KOR_NM, ENG_NM)
        match_columns = ['KOR_NM', 'ENG_NM']
        
        # ê° ì»¬ëŸ¼ì—ì„œ ë§¤ì¹­ ì‹œë„
        matched_row = None
        for col in match_columns:
            if col in self.product_ref_df.columns:
                # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í–‰ ì°¾ê¸° (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)
                mask = self.product_ref_df[col].astype(str).str.strip().str.lower() == str(product_type).strip().lower()
                if mask.any():
                    matched_row = self.product_ref_df[mask].iloc[0]
                    break
        
        if matched_row is not None:
            # ì¶œë ¥ í•„ë“œ ì¶”ì¶œ: HTRK_PRDLST_CD, HRRK_PRDLST_CD
            info["top"] = matched_row.get("HTRK_PRDLST_CD") if "HTRK_PRDLST_CD" in matched_row.index else None
            info["upper"] = matched_row.get("HRRK_PRDLST_CD") if "HRRK_PRDLST_CD" in matched_row.index else None
        
        return info

    def _lookup_hazard_info(self, hazard_item):
        """
        ì‹œí—˜í•­ëª© ì´ë¦„ìœ¼ë¡œ ë¶„ë¥˜(ì¹´í…Œê³ ë¦¬) ì¡°íšŒ
        
        Logic 2: Hazard Classification Lookup
        - Input: hazard_item (from API)
        - Reference: hazard_code_master.parquet
        - Matching Rule: Find row where hazard_item matches ANY of:
          ['KOR_NM', 'ENG_NM', 'ABRV', 'NCKNM', 'TESTITM_NM']
        - Output Mapping:
          - hazard_category â† M_KOR_NM (from reference)
          - analyzable â† ANALYZABLE (from reference)
          - interest_item â† INTEREST_ITEM (from reference)
        """
        info = {"category": None, "analyzable": False, "interest": False}
        
        if self.hazard_ref_df.empty or not hazard_item:
            return info
        
        # ë§¤ì¹­í•  ì»¬ëŸ¼ë“¤
        match_columns = ['KOR_NM', 'ENG_NM', 'ABRV', 'NCKNM', 'TESTITM_NM']
        
        # ê° ì»¬ëŸ¼ì—ì„œ ë§¤ì¹­ ì‹œë„
        matched_row = None
        for col in match_columns:
            if col in self.hazard_ref_df.columns:
                # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í–‰ ì°¾ê¸° (ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´)
                mask = self.hazard_ref_df[col].astype(str).str.strip().str.lower() == str(hazard_item).strip().lower()
                if mask.any():
                    matched_row = self.hazard_ref_df[mask].iloc[0]
                    break
        
        if matched_row is not None:
            # ì¶œë ¥ í•„ë“œ ì¶”ì¶œ: M_KOR_NM, ANALYZABLE, INTEREST_ITEM
            info["category"] = matched_row.get("M_KOR_NM") if "M_KOR_NM" in matched_row.index else None
            info["analyzable"] = bool(matched_row.get("ANALYZABLE", False)) if "ANALYZABLE" in matched_row.index else False
            info["interest"] = bool(matched_row.get("INTEREST_ITEM", False)) if "INTEREST_ITEM" in matched_row.index else False
        
        return info

    def collect_i2620(self):
        """
        [I2620] êµ­ë‚´ì‹í’ˆ ê²€ì‚¬ë¶€ì í•© ìˆ˜ì§‘ ë¡œì§
        """
        service_id = "I2620"
        print(f"ğŸš€ [I2620] êµ­ë‚´ì‹í’ˆ ë¶€ì í•© ì •ë³´ ìˆ˜ì§‘ ì‹œì‘...")
        
        all_records = []
        start, step = 1, 1000
        
        while True:
            end = start + step - 1
            rows = self.fetch_service(service_id, start, end)
            
            if not rows:
                print(f"   ğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ (Total pages processed)")
                break
                
            print(f"   - Processing {start} ~ {end} (Got {len(rows)} items)")
            
            for row in rows:
                try:
                    # 1. í•„ë“œ ì¶”ì¶œ (API ëª…ì„¸ ê¸°ì¤€)
                    raw_date = row.get("CRET_DTM", "") # ë“±ë¡ì¼ (YYYY.MM.DD)
                    product_name = row.get("PRDTNM", "") # ì œí’ˆëª…
                    product_type = row.get("PRDLST_CD_NM", "") # ì‹í’ˆìœ í˜• (ex: ëƒ‰ì´)
                    hazard_item = row.get("TEST_ITMNM", "") # ë¶€ì í•©í•­ëª© (ex: íœë””ë©”íƒˆë¦°)
                    unique_seq = row.get("RTRVLDSUSE_SEQ", "") # íšŒìˆ˜íê¸°ì¼ë ¨ë²ˆí˜¸
                    
                    # 2. ë°ì´í„° ì •ì œ & Lookup
                    reg_date = self.normalize_date(raw_date)
                    prod_info = self._lookup_product_info(product_type)
                    hazard_info = self._lookup_hazard_info(hazard_item)
                    
                    # 3. ìƒì„¸ ì¶œì²˜ ìƒì„±
                    source_detail = f"{service_id}-{unique_seq}" if unique_seq else f"{service_id}-UNKNOWN"
                    
                    # 4. í†µí•© ìŠ¤í‚¤ë§ˆ ë§¤í•‘ (13 Columns Strict)
                    record = {
                        "registration_date": reg_date,
                        "data_source": "MFDS",
                        "source_detail": source_detail,
                        "product_type": product_type,
                        "top_level_product_type": prod_info["top"],
                        "upper_product_type": prod_info["upper"],
                        "product_name": product_name,
                        "origin_country": "South Korea", # êµ­ë‚´ì‹í’ˆ
                        "notifying_country": "South Korea",
                        "hazard_category": hazard_info["category"],
                        "hazard_item": hazard_item,
                        "analyzable": hazard_info["analyzable"],
                        "interest_item": hazard_info["interest"]
                    }
                    all_records.append(record)
                    
                except Exception as e:
                    print(f"   âš ï¸ Skipping row due to error: {e}")
                    continue

            # í…ŒìŠ¤íŠ¸ìš©: ë„ˆë¬´ ë§ìœ¼ë©´ ì˜¤ë˜ ê±¸ë¦¬ë¯€ë¡œ ì¼ë‹¨ 2000ê±´ì—ì„œ break (ì‹¤ì œ ìš´ì˜ ì‹œ ì œê±°)
            # if end >= 2000: break 
            
            start += step

        if not all_records:
            return get_empty_dataframe()
            
        return pd.DataFrame(all_records)

    def collect_i0490(self):
        """
        [I0490] íšŒìˆ˜íŒë§¤ì¤‘ì§€ ì •ë³´ ìˆ˜ì§‘ ë¡œì§
        """
        service_id = "I0490"
        print(f"ğŸš€ [I0490] íšŒìˆ˜íŒë§¤ì¤‘ì§€ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘...")
        
        all_records = []
        start, step = 1, 1000
        
        while True:
            end = start + step - 1
            rows = self.fetch_service(service_id, start, end)
            
            if not rows:
                print(f"   ğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ (Total pages processed)")
                break
                
            print(f"   - Processing {start} ~ {end} (Got {len(rows)} items)")
            
            for row in rows:
                try:
                    # 1. í•„ë“œ ì¶”ì¶œ (API ëª…ì„¸ ê¸°ì¤€)
                    raw_date = row.get("CRET_DTM", "")  # ë“±ë¡ì¼ (YYYY-MM-DD HH:MM:SS)
                    product_name = row.get("PRDTNM", "")  # ì œí’ˆëª…
                    product_type = row.get("PRDLST_CD_NM", "")  # ì‹í’ˆìœ í˜•
                    recall_reason = row.get("RTRVLPRVNS", "")  # íšŒìˆ˜ì‚¬ìœ  (e.g., ì´ë¬¼ í˜¼ì…)
                    unique_seq = row.get("RTRVLDSUSE_SEQ", "")  # íšŒìˆ˜íê¸°ì¼ë ¨ë²ˆí˜¸
                    
                    # 2. ë‚ ì§œ ì •ê·œí™”: YYYY-MM-DD HH:MM:SS -> YYYY-MM-DD (ì²« 10ê¸€ìë§Œ)
                    reg_date = raw_date[:10] if raw_date else None
                    
                    # 3. ë°ì´í„° ì •ì œ & Lookup
                    prod_info = self._lookup_product_info(product_type)
                    hazard_info = self._lookup_hazard_info(recall_reason)
                    
                    # 4. ìƒì„¸ ì¶œì²˜ ìƒì„±
                    source_detail = f"{service_id}-{unique_seq}" if unique_seq else f"{service_id}-UNKNOWN"
                    
                    # 5. í†µí•© ìŠ¤í‚¤ë§ˆ ë§¤í•‘ (13 Columns Strict)
                    record = {
                        "registration_date": reg_date,
                        "data_source": "MFDS",
                        "source_detail": source_detail,
                        "product_type": product_type,
                        "top_level_product_type": prod_info["top"],
                        "upper_product_type": prod_info["upper"],
                        "product_name": product_name,
                        "origin_country": "South Korea",  # êµ­ë‚´ì‹í’ˆ íšŒìˆ˜
                        "notifying_country": "South Korea",
                        "hazard_category": hazard_info["category"],
                        "hazard_item": recall_reason,
                        "analyzable": hazard_info["analyzable"],
                        "interest_item": hazard_info["interest"]
                    }
                    all_records.append(record)
                    
                except Exception as e:
                    print(f"   âš ï¸ Skipping row due to error: {e}")
                    continue

            start += step

        if not all_records:
            return get_empty_dataframe()
            
        return pd.DataFrame(all_records)

    def _load_country_reference(self):
        """êµ­ê°€ëª… ë§ˆìŠ¤í„° TSV -> ë”•ì…”ë„ˆë¦¬ ë³€í™˜"""
        file_path = self.REF_DIR / "country_master.tsv"
        if not file_path.exists():
            print(f"   âš ï¸ Warning: country_master.tsv íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        try:
            # TSV íŒŒì¼ì„ ì½ê¸° (íƒ­ êµ¬ë¶„ì)
            df = pd.read_csv(file_path, sep='\t', encoding='utf-8')
            
            # êµ­ê°€ëª…(í•œê¸€)ì„ í‚¤ë¡œ, ì˜ë¬¸ëª…+ISOë¥¼ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ìƒì„±
            country_dict = {}
            for _, row in df.iterrows():
                kor_name = row.get('êµ­ê°€ëª…(êµ­ë¬¸)', '')
                if kor_name and pd.notna(kor_name):
                    country_dict[kor_name] = {
                        'eng': row.get('êµ­ê°€ëª…(ì˜ë¬¸)', ''),
                        'iso_2': row.get('ISO(2ìë¦¬)', ''),
                        'iso_3': row.get('ISO(3ìë¦¬)', '')
                    }
            return country_dict
        except Exception as e:
            print(f"   âŒ country_master.tsv ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {}

    def _normalize_country_name(self, raw_country):
        """ì›ë³¸ êµ­ê°€ëª…(BDTì—ì„œ ì¶”ì¶œ)ì„ ì •ê·œí™”ëœ êµ­ê°€ëª…ìœ¼ë¡œ ë³€í™˜"""
        if not raw_country:
            return "Overseas"
        
        # ì¢Œìš° ê³µë°± ì œê±°
        raw_country = raw_country.strip()
        
        # 1ì°¨: ì •í™•í•œ ë§¤ì¹˜
        if raw_country in self.country_ref:
            return self.country_ref[raw_country]['eng']
        
        # 2ì°¨: ë¶€ë¶„ ë§¤ì¹˜ (ì²« ë¬¸ì ì¼ì¹˜)
        for kor_name, data in self.country_ref.items():
            if kor_name.startswith(raw_country[:2]):  # ì²« 2ê¸€ì ì¼ì¹˜
                return data['eng']
        
        # 3ì°¨: ë°˜í™˜ (ë§¤ì¹­ ì‹¤íŒ¨)
        return raw_country if raw_country else "Overseas"

    def _extract_origin_from_bdt(self, bdt_text):
        """BDT í•„ë“œì—ì„œ ì§€ì—­(ì›ì‚°ì§€) ì •ë³´ ì •ê·œì‹ ì¶”ì¶œ"""
        if not bdt_text:
            return "Overseas"
        
        # íŒ¨í„´ 1: "-ì§€ì—­: êµ­ê°€ëª…" ë˜ëŠ” "ì§€ì—­: êµ­ê°€ëª…"
        match = re.search(r"[-]?ì§€ì—­:\s*([ê°€-í£a-zA-Z\s]+?)(?:\s*[-]|$)", bdt_text)
        if match:
            origin = match.group(1).strip()
            return origin if origin else "Overseas"
        
        # íŒ¨í„´ 2: "ì§€ì—­" í‚¤ì›Œë“œ ë’¤ì˜ í…ìŠ¤íŠ¸
        match = re.search(r"ì§€ì—­\s*[:\-]\s*([ê°€-í£a-zA-Z]+)", bdt_text)
        if match:
            return match.group(1).strip()
        
        # ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
        return "Overseas"

    def collect_i2810(self):
        """
        [I2810] í•´ì™¸ ìœ„í•´ì‹í’ˆ íšŒìˆ˜ì •ë³´ ìˆ˜ì§‘ ë¡œì§
        ë°ì´í„°ê°€ BDT í•„ë“œì˜ ë¹„ì •í˜• í…ìŠ¤íŠ¸ì— í¬í•¨ë˜ì–´ ìˆì–´ ì •ê·œì‹ íŒŒì‹±ì´ í•„ìš”í•¨
        """
        service_id = "I2810"
        print(f"ğŸš€ [I2810] í•´ì™¸ ìœ„í•´ì‹í’ˆ íšŒìˆ˜ì •ë³´ ìˆ˜ì§‘ ì‹œì‘...")
        
        all_records = []
        start, step = 1, 1000
        
        while True:
            end = start + step - 1
            rows = self.fetch_service(service_id, start, end)
            
            if not rows:
                print(f"   ğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ (Total pages processed)")
                break
                
            print(f"   - Processing {start} ~ {end} (Got {len(rows)} items)")
            
            for row in rows:
                try:
                    # 1. í•„ë“œ ì¶”ì¶œ
                    raw_date = row.get("CRET_DTM", "")  # ë“±ë¡ì¼ (YYYYMMDD)
                    product_name = row.get("TITL", "")  # ì œí’ˆëª…
                    hazard_item = row.get("DETECT_TITL", "")  # ìœ„í•´ë¬¼ì§ˆ
                    notify_no = row.get("NTCTXT_NO", "")  # í†µì§€ë²ˆí˜¸
                    bdt_text = row.get("BDT", "")  # ë¹„ì •í˜• í…ìŠ¤íŠ¸ (ì§€ì—­ ì¶”ì¶œ ëŒ€ìƒ)
                    
                    # 2. ë‚ ì§œ ì •ê·œí™”: YYYYMMDD -> YYYY-MM-DD
                    if raw_date and len(raw_date) == 8:
                        reg_date = f"{raw_date[0:4]}-{raw_date[4:6]}-{raw_date[6:8]}"
                    else:
                        reg_date = None
                    
                    # 3. BDTì—ì„œ ì›ì‚°ì§€ ì¶”ì¶œ (ì •ê·œì‹)
                    origin_country = self._extract_origin_from_bdt(bdt_text)
                    
                    # 4. Lookupì„ í†µí•œ ë¶„ë¥˜ ì •ë³´ ì¡°íšŒ
                    # ì œí’ˆìœ í˜•ì€ ê³ ì •ê°’ì´ë¯€ë¡œ lookup ìŠ¤í‚µ
                    hazard_info = self._lookup_hazard_info(hazard_item)
                    
                    # 5. ìƒì„¸ ì¶œì²˜ ìƒì„±
                    source_detail = f"{service_id}-{notify_no}" if notify_no else f"{service_id}-UNKNOWN"
                    
                    # 6. í†µí•© ìŠ¤í‚¤ë§ˆ ë§¤í•‘ (13 Columns Strict)
                    record = {
                        "registration_date": reg_date,
                        "data_source": "MFDS",
                        "source_detail": source_detail,
                        "product_type": "ìˆ˜ì…ì‹í’ˆ(í•´ì™¸íšŒìˆ˜)",  # ê³ ì •ê°’
                        "top_level_product_type": "ìˆ˜ì…ì‹í’ˆ",  # ê³ ì •ê°’
                        "upper_product_type": "ìœ„í•´íšŒìˆ˜",  # ê³ ì •ê°’
                        "product_name": product_name,
                        "origin_country": origin_country,  # BDTì—ì„œ ì¶”ì¶œ
                        "notifying_country": "South Korea",  # ê³ ì •ê°’ (MFDS)
                        "hazard_category": hazard_info["category"],
                        "hazard_item": hazard_item,
                        "analyzable": hazard_info["analyzable"],
                        "interest_item": hazard_info["interest"]
                    }
                    all_records.append(record)
                    
                except Exception as e:
                    print(f"   âš ï¸ Skipping row due to error: {e}")
                    continue

            start += step

        if not all_records:
            return get_empty_dataframe()
            
        return pd.DataFrame(all_records)

    def collect(self):
        """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜: ëª¨ë“  MFDS ì„œë¹„ìŠ¤ í†µí•© ìˆ˜ì§‘"""
        # 1. ê° ì„œë¹„ìŠ¤ë³„ ìˆ˜ì§‘
        df_i2620 = self.collect_i2620()
        df_i0490 = self.collect_i0490()
        df_i2810 = self.collect_i2810()
        
        # 2. ê²°ê³¼ ë³‘í•©
        dfs_to_combine = [df for df in [df_i2620, df_i0490, df_i2810] if not df.empty]
        
        if not dfs_to_combine:
            return get_empty_dataframe()
        
        combined_df = pd.concat(dfs_to_combine, ignore_index=True)
        
        # 3. ìµœì¢… ìŠ¤í‚¤ë§ˆ ê²€ì¦ ë° ë°˜í™˜
        final_df = validate_schema(combined_df)
        print(f"âœ… [Total] ì´ {len(final_df)} ê±´ ìˆ˜ì§‘ ë° ì •ê·œí™” ì™„ë£Œ (I2620 + I0490 + I2810).")
        return final_df

if __name__ == "__main__":
    collector = MFDSCollector()
    df = collector.collect()
    print(df.head(5))
    
    # ê²°ê³¼ í™•ì¸ìš© ì €ì¥
    # df.to_csv("mfds_i2620_result.csv", index=False, encoding='utf-8-sig')
import argparse
import time
import schedule
from pathlib import Path
from datetime import datetime
from loguru import logger
import sys

# Add parent directory to path for imports when running as script
if __name__ == '__main__':
    sys.path.insert(0, str(Path(__file__).parent.parent))

# Import Collectors
from src.collectors.mfds_collector import MFDSCollector
from src.collectors.fda_collector import FDACollector
from src.collectors.rasff_scraper import RASFFCollector
from src.collectors.impfood_scraper import ImpFoodScraper

# Import Utils
from src.utils.deduplication import merge_and_deduplicate
from src.utils.storage import save_to_parquet

class DataIngestionScheduler:
    """
    ì¤‘ì•™ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬
    - ëª¨ë“  ìˆ˜ì§‘ê¸°(MFDS, FDA, RASFF)ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
    - ìˆ˜ì§‘ -> ì •ì œ -> ì¤‘ë³µì œê±° -> ì €ì¥ íŒŒì´í”„ë¼ì¸ ê´€ë¦¬
    """
    
    def __init__(self, data_dir: Path = Path("data/hub")):
        self.data_dir = data_dir
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # ìˆ˜ì§‘ê¸° ì¸ìŠ¤í„´ìŠ¤ ì´ˆê¸°í™”
        self.collectors = {
            "MFDS": MFDSCollector(),
            "FDA": FDACollector(),
            "RASFF": RASFFCollector(),
            "ImpFood": ImpFoodScraper()
        }

    def run_single_collector(self, name):
        """ë‹¨ì¼ ìˆ˜ì§‘ê¸° ì‹¤í–‰ ë° ì €ì¥"""
        logger.info(f"ğŸš€ Starting Collector: {name}")
        try:
            collector = self.collectors[name]
            
            # ìˆ˜ì§‘ ì‹¤í–‰ (ê° í´ë˜ìŠ¤ì˜ ë©”ì¸ ë©”ì„œë“œ í˜¸ì¶œ)
            if name in ["RASFF", "ImpFood"]:
                df = collector.scrape() # RASFFì™€ ImpFoodëŠ” scrape() ë©”ì„œë“œ ì‚¬ìš©
            else:
                df = collector.collect() # ë‚˜ë¨¸ì§€ëŠ” collect() ì‚¬ìš©
                
            if df.empty:
                logger.info(f"âš ï¸ {name}: No data collected.")
                return 0
                
            # ì¤‘ë³µ ì œê±°
            df_new = merge_and_deduplicate(df, self.data_dir)
            
            # ì €ì¥
            count = save_to_parquet(df_new, self.data_dir, name)
            logger.success(f"âœ… {name}: {count} new records saved.")
            return count
            
        except Exception as e:
            logger.error(f"âŒ {name} Failed: {e}")
            return 0

    def run_all_collectors(self):
        """ëª¨ë“  ìˆ˜ì§‘ê¸° ìˆœì°¨ ì‹¤í–‰"""
        logger.info("ğŸ”„ Running ALL Collectors...")
        total_new = 0
        for name in self.collectors:
            total_new += self.run_single_collector(name)
        logger.success(f"ğŸ‰ Pipeline Finished. Total new records: {total_new}")
        return total_new

def job():
    """ìŠ¤ì¼€ì¤„ëŸ¬ì— ë“±ë¡ë  ì‘ì—…"""
    print(f"\n[Scheduler] Job started at {datetime.now()}")
    scheduler = DataIngestionScheduler()
    scheduler.run_all_collectors()

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--mode", choices=["once", "schedule"], required=True, help="Run mode")
    parser.add_argument("--time", default="09:00", help="Time to run in schedule mode (HH:MM)")
    parser.add_argument("--days", type=int, default=7, help="Days back (not used in all collectors)")
    
    args = parser.parse_args()
    
    if args.mode == "once":
        job()
    elif args.mode == "schedule":
        logger.info(f"â° Scheduler started. Running daily at {args.time}")
        schedule.every().day.at(args.time).do(job)
        
        while True:
            schedule.run_pending()
            time.sleep(60)

if __name__ == "__main__":
    main()